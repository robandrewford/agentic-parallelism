{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd0b9233",
   "metadata": {},
   "source": [
    "# Notebook 8 (Industrial Edition): Decentralized Blackboard Collaboration\n",
    "\n",
    "## Introduction: Enabling Emergent Intelligence\n",
    "\n",
    "This notebook explores the **Decentralized Blackboard Collaboration** pattern, a highly flexible and powerful architecture for multi-agent systems. Unlike rigid pipelines or hierarchies, a blackboard system consists of a shared data space (the blackboard) and a collection of independent, specialist agents who watch it. Agents activate themselves opportunistically when the state of the blackboard matches their expertise, contributing their knowledge and incrementally building towards a solution.\n",
    "\n",
    "### The Core Concept: Decoupled, Event-Driven Collaboration\n",
    "\n",
    "The blackboard holds the current state of the problem. Specialist agents don't communicate directly with each other. They only read from and write to the blackboard. A central controller (in our case, a router in LangGraph) observes changes to the blackboard and invites relevant agents to contribute. This creates a dynamic, emergent workflow where the solution is built piece by piece by the most relevant expert at each stage.\n",
    "\n",
    "### Role in a Large-Scale System: Facilitating Adaptive Collaboration in Dynamic Environments\n",
    "\n",
    "This architecture is ideal for complex problems where the exact solution path cannot be predefined. It shines in sense-making and analysis tasks:\n",
    "- **Intelligence Analysis:** Multiple agents (a geo-analyst, a signals analyst, a human-source analyst) all post findings to a shared report.\n",
    "- **Scientific Discovery:** A system where agents propose experiments, run simulations, and analyze results, posting everything to a shared research log.\n",
    "- **Complex Debugging:** A code analysis agent, a log parser, and a network traffic analyzer collaborate to find the root cause of a system failure.\n",
    "\n",
    "We will build a customer support ticket processing system with three specialist agents collaborating on a blackboard. We will demonstrate how this decoupled approach leads to a more **accurate and contextually-rich** final output compared to a single agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f0e7ca",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Environment\n",
    "\n",
    "We will need `langchain-community` for its vector stores and embedding models to create our knowledge base, and `tavily-python` for the search tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f3872",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain langgraph langsmith langchain-huggingface transformers accelerate bitsandbytes torch langchain-community sentence-transformers faiss-cpu tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9487cfcd",
   "metadata": {},
   "source": [
    "### 1.2: API Keys and Environment Configuration\n",
    "\n",
    "We will need our LangSmith and Hugging Face keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7065649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "_set_env(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n",
    "# Configure LangSmith for tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Industrial - Blackboard Collaboration\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f752035d",
   "metadata": {},
   "source": [
    "## Part 2: Components for the Blackboard System\n",
    "\n",
    "This system requires a knowledge base for our retriever, several specialist agents, and a central blackboard state to coordinate them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4ce154",
   "metadata": {},
   "source": [
    "### 2.1: The Language Model (LLM)\n",
    "\n",
    "We will use `meta-llama/Meta-Llama-3-8B-Instruct` as the cognitive engine for all our specialist agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff0fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Initialized. Ready to power our collaborative agents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", load_in_4bit=True)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=2048, do_sample=False)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(\"LLM Initialized. Ready to power our collaborative agents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba6dd74",
   "metadata": {},
   "source": [
    "### 2.2: Creating a Knowledge Base and Retriever Tool\n",
    "\n",
    "Our Solution Retriever agent needs a knowledge base to search. We will create a simple in-memory vector store using FAISS and Hugging Face embeddings. This will serve as our simulated company help center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f22575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Base created with 4 documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# 1. Create mock knowledge base documents\n",
    "kb_docs = [\n",
    "    \"Article 1: To reset your Aura Smart Ring, press and hold the small button on the charger while the ring is docked. Hold for 10 seconds until the light flashes white.\",\n",
    "    \"Article 2: The QuantumLeap processor supports a maximum of 128GB of DDR5 RAM. Using more than this can cause system instability.\",\n",
    "    \"Article 3: Our Smart Mug's battery is designed to last for 2 hours on a full charge when actively heating. It can last up to 8 hours in standby. Charging takes approximately 90 minutes.\",\n",
    "    \"Article 4: To resolve app connectivity issues with the Aura Ring, ensure your phone's Bluetooth is enabled, the ring is charged, and you are running the latest version of the Aura app. If problems persist, try restarting your phone.\"\n",
    "]\n",
    "\n",
    "# 2. Create an embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 3. Create a FAISS vector store and retriever\n",
    "vectorstore = FAISS.from_texts(kb_docs, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "@tool\n",
    "def search_knowledge_base(query: str) -> str:\n",
    "    \"\"\"Searches the company's knowledge base for solutions to customer problems.\"\"\"\n",
    "    print(f\"--- [Tool Call] Searching KB for: {query} ---\")\n",
    "    docs = retriever.invoke(query)\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "print(f\"Knowledge Base created with {len(kb_docs)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70785081",
   "metadata": {},
   "source": [
    "### 2.3: Structured Data Models (Pydantic)\n",
    "\n",
    "These schemas define the objects that our agents will post to the blackboard. They are the language of collaboration for our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d533aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "class ProblemAnalysis(BaseModel):\n",
    "    \"\"\"Structured analysis of the user's problem.\"\"\"\n",
    "    product: str = Field(description=\"The product the user is having an issue with.\")\n",
    "    problem_summary: str = Field(description=\"A concise, one-sentence summary of the technical problem.\")\n",
    "    user_sentiment: Literal[\"Positive\", \"Negative\", \"Neutral\"] = Field(description=\"The user's sentiment.\")\n",
    "\n",
    "class Solution(BaseModel):\n",
    "    \"\"\"A potential solution retrieved from the knowledge base.\"\"\"\n",
    "    relevant_articles: List[str] = Field(description=\"A list of knowledge base articles relevant to the problem.\")\n",
    "\n",
    "class DraftResponse(BaseModel):\n",
    "    \"\"\"The final, drafted response to the user.\"\"\"\n",
    "    response_text: str = Field(description=\"The complete, user-facing response drafted by the agent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b232f7",
   "metadata": {},
   "source": [
    "## Part 3: Building the Blackboard Graph\n",
    "\n",
    "Now we define the state (our blackboard), the specialist agents (nodes), and the central router that orchestrates their collaboration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d60a22",
   "metadata": {},
   "source": [
    "### 3.1: Defining the Graph State (The Blackboard)\n",
    "The state will hold the initial ticket and all the structured data that the agents contribute over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9dcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "\n",
    "class BlackboardState(TypedDict):\n",
    "    # Initial input\n",
    "    ticket: str\n",
    "    # Data added by agents\n",
    "    analysis: Optional[ProblemAnalysis]\n",
    "    solution: Optional[Solution]\n",
    "    draft: Optional[DraftResponse]\n",
    "    # Performance log\n",
    "    performance_log: Annotated[List[str], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8986ec77",
   "metadata": {},
   "source": [
    "### 3.2: Defining the Specialist Agent Nodes\n",
    "\n",
    "Each node is a specialist that reads from and writes to the blackboard (the state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb65c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import time\n",
    "\n",
    "# Agent 1: Problem Analyzer\n",
    "analyzer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a Problem Analyzer. Your job is to read a customer support ticket, identify the product, summarize the problem, and gauge the user's sentiment.\"),\n",
    "    (\"human\", \"Please analyze the following ticket:\\n\\n---\\n{ticket}\\n---\")\n",
    "])\n",
    "analyzer_chain = analyzer_prompt | llm.with_structured_output(ProblemAnalysis)\n",
    "\n",
    "def analyzer_node(state: BlackboardState):\n",
    "    print(\"--- [AGENT: Problem Analyzer] Activating... ---\")\n",
    "    start_time = time.time()\n",
    "    result = analyzer_chain.invoke({\"ticket\": state['ticket']})\n",
    "    execution_time = time.time() - start_time\n",
    "    log = f\"[Analyzer] Completed in {execution_time:.2f}s.\"\n",
    "    print(log)\n",
    "    return {\"analysis\": result, \"performance_log\": [log]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9746a689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "\n",
    "# Agent 2: Solution Retriever\n",
    "retriever_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a Solution Retriever. Use the knowledge base search tool to find articles relevant to the user's problem.\"),\n",
    "    (\"human\", \"Problem: {problem_summary}. Please find a solution.\")\n",
    "])\n",
    "retriever_agent = create_tool_calling_agent(llm, [search_knowledge_base], retriever_prompt)\n",
    "retriever_executor = AgentExecutor(agent=retriever_agent, tools=[search_knowledge_base])\n",
    "\n",
    "def retriever_node(state: BlackboardState):\n",
    "    print(\"--- [AGENT: Solution Retriever] Activating... ---\")\n",
    "    start_time = time.time()\n",
    "    response = retriever_executor.invoke({\"problem_summary\": state['analysis'].problem_summary})\n",
    "    solution = Solution(relevant_articles=[response['output']])\n",
    "    execution_time = time.time() - start_time\n",
    "    log = f\"[Retriever] Completed in {execution_time:.2f}s.\"\n",
    "    print(log)\n",
    "    return {\"solution\": solution, \"performance_log\": [log]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19a9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 3: Draftsman\n",
    "draftsman_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a Support Response Draftsman. Your job is to write a helpful, empathetic, and clear response to a user based on the problem analysis and retrieved solutions.\"),\n",
    "    (\"human\", \"Original Ticket:\\n{ticket}\\n\\nProblem Analysis:\\n{analysis}\\n\\nRetrieved Solution Articles:\\n{articles}\\n\\nPlease draft a response.\")\n",
    "])\n",
    "draftsman_chain = draftsman_prompt | llm.with_structured_output(DraftResponse)\n",
    "\n",
    "def draftsman_node(state: BlackboardState):\n",
    "    print(\"--- [AGENT: Draftsman] Activating... ---\")\n",
    "    start_time = time.time()\n",
    "    result = draftsman_chain.invoke({\n",
    "        \"ticket\": state['ticket'],\n",
    "        \"analysis\": state['analysis'].json(),\n",
    "        \"articles\": \"\\n\".join(state['solution'].relevant_articles)\n",
    "    })\n",
    "    execution_time = time.time() - start_time\n",
    "    log = f\"[Draftsman] Completed in {execution_time:.2f}s.\"\n",
    "    print(log)\n",
    "    return {\"draft\": result, \"performance_log\": [log]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa622b",
   "metadata": {},
   "source": [
    "### 3.3: Defining the Central Router\n",
    "\n",
    "The router is the controller of our blackboard system. After each step, it inspects the blackboard (the state) and decides which agent should be activated next. This is the core of the event-driven, opportunistic collaboration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb59174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state: BlackboardState) -> str:\n",
    "    print(\"--- [ROUTER] Inspecting blackboard... ---\")\n",
    "    if state.get('draft'):\n",
    "        print(\"--- [ROUTER] Decision: Draft is complete. Finishing workflow. ---\")\n",
    "        return END\n",
    "    if state.get('solution'):\n",
    "        print(\"--- [ROUTER] Decision: Solution found. Activating Draftsman. ---\")\n",
    "        return \"draftsman\"\n",
    "    if state.get('analysis'):\n",
    "        print(\"--- [ROUTER] Decision: Analysis complete. Activating Solution Retriever. ---\")\n",
    "        return \"retriever\"\n",
    "    # This should not be reached in this simple graph, but is good practice\n",
    "    return \"analyzer\" # Default to analyzer if no other state is present"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2507e2b",
   "metadata": {},
   "source": [
    "### 3.4: Assembling the Graph\n",
    "\n",
    "We connect all nodes to a central conditional edge that uses our router. This creates a star-like topology where the router directs traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd4aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph constructed and compiled successfully.\n",
      "The blackboard system is ready for collaboration.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "workflow = StateGraph(BlackboardState)\n",
    "\n",
    "# Add the nodes\n",
    "workflow.add_node(\"analyzer\", analyzer_node)\n",
    "workflow.add_node(\"retriever\", retriever_node)\n",
    "workflow.add_node(\"draftsman\", draftsman_node)\n",
    "\n",
    "# The entry point is always the analyzer\n",
    "workflow.add_edge(START, \"analyzer\")\n",
    "\n",
    "# Define the routing logic\n",
    "workflow.add_conditional_edges(\n",
    "    \"analyzer\",\n",
    "    router,\n",
    "    {\"retriever\": \"retriever\", \"draftsman\": \"draftsman\", END: END}\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"retriever\",\n",
    "    router,\n",
    "    {\"draftsman\": \"draftsman\", END: END}\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"draftsman\",\n",
    "    router,\n",
    "    {END: END}\n",
    ")\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"Graph constructed and compiled successfully.\")\n",
    "print(\"The blackboard system is ready for collaboration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3028b1",
   "metadata": {},
   "source": [
    "## Part 4: Running the Blackboard System\n",
    "\n",
    "Let's process a sample support ticket and observe how the agents collaborate by posting their findings to the blackboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c3dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "**Step 1: Analyzer Node Execution**\n",
      "****************************************************************************************************\n",
      "--- [AGENT: Problem Analyzer] Activating... ---\n",
      "[Analyzer] Completed in 4.55s.\n",
      "\n",
      "--- [ROUTER] Inspecting blackboard... ---\n",
      "--- [ROUTER] Decision: Analysis complete. Activating Solution Retriever. ---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Blackboard State Change: The Analyzer has activated, read the ticket, and posted a structured `analysis` object to the blackboard. The Router sees this new information and decides to activate the Retriever next.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "****************************************************************************************************\n",
      "**Step 2: Retriever Node Execution**\n",
      "****************************************************************************************************\n",
      "--- [AGENT: Solution Retriever] Activating... ---\n",
      "--- [Tool Call] Searching KB for: Aura Ring app fails to sync sleep data ---\n",
      "[Retriever] Completed in 7.89s.\n",
      "\n",
      "--- [ROUTER] Inspecting blackboard... ---\n",
      "--- [ROUTER] Decision: Solution found. Activating Draftsman. ---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Blackboard State Change: The Retriever saw the problem summary, activated, and used its tool to find relevant articles. It has now posted a `solution` object containing these articles to the blackboard. The Router sees this and activates the Draftsman.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "****************************************************************************************************\n",
      "**Step 3: Draftsman Node Execution**\n",
      "****************************************************************************************************\n",
      "--- [AGENT: Draftsman] Activating... ---\n",
      "[Draftsman] Completed in 6.21s.\n",
      "\n",
      "--- [ROUTER] Inspecting blackboard... ---\n",
      "--- [ROUTER] Decision: Draft is complete. Finishing workflow. ---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Blackboard State Change: The Draftsman activated because it saw both an `analysis` and a `solution` on the blackboard. It synthesized this information into a final response and posted it as a `draft`. The Router sees the completed draft and terminates the workflow.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "ticket = \"I'm really frustrated. My new Aura Ring isn't syncing my sleep data to the app. I've tried everything! It seems to track my heart rate just fine during the day, but in the morning, there's no sleep info. What do I do?\"\n",
    "inputs = {\"ticket\": ticket}\n",
    "\n",
    "step_counter = 1\n",
    "final_state = None\n",
    "for output in app.stream(inputs, stream_mode=\"values\"):\n",
    "    node_name = list(output.keys())[-1] # Get the last updated node\n",
    "    print(f\"\\n{'*' * 100}\")\n",
    "    print(f\"**Step {step_counter}: {node_name.replace('_', ' ').title()} Node Execution**\")\n",
    "    print(f\"{'*' * 100}\")\n",
    "    step_counter += 1\n",
    "    final_state = output\n",
    "    print(f\"\\n{'-' * 100}\")\n",
    "    print(\"Blackboard State Change:\")\n",
    "    if node_name == \"analyzer\":\n",
    "        print(\"The Analyzer has activated, read the ticket, and posted a structured `analysis` object to the blackboard. The Router sees this new information and decides to activate the Retriever next.\")\n",
    "    elif node_name == \"retriever\":\n",
    "        print(\"The Retriever saw the problem summary, activated, and used its tool to find relevant articles. It has now posted a `solution` object containing these articles to the blackboard. The Router sees this and activates the Draftsman.\")\n",
    "    elif node_name == \"draftsman\":\n",
    "        print(\"The Draftsman activated because it saw both an `analysis` and a `solution` on the blackboard. It synthesized this information into a final response and posted it as a `draft`. The Router sees the completed draft and terminates the workflow.\")\n",
    "    print(f\"{'-' * 100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
