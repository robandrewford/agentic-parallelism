{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0e9472",
   "metadata": {},
   "source": [
    "# Notebook 2 (Industrial Edition): Parallel Hypothesis Generation\n",
    "\n",
    "## Introduction: Moving from Execution to Strategy\n",
    "\n",
    "This notebook explores a more advanced and powerful parallelism pattern: **Parallel Hypothesis Generation**, also known as Branching Thoughts. This architecture elevates an agent from a simple task-executor to a strategic thinker. Instead of following a single path, the system generates multiple potential strategies (hypotheses), explores them all simultaneously, and then synthesizes the results to find the optimal solution.\n",
    "\n",
    "### Why is this a critical pattern?\n",
    "\n",
    "For simple, well-defined problems, a single line of reasoning is sufficient. But for complex, ambiguous, or creative tasks, the first idea is rarely the best. A human strategist would brainstorm multiple approaches before diving in. This pattern enables our AI systems to do the same, preventing them from getting stuck on a suboptimal path and dramatically increasing the quality of the final output.\n",
    "\n",
    "### Role in a Large-Scale System: Tackling Complex & Ambiguous Problems\n",
    "\n",
    "In an industrial setting, this pattern is essential for any system that needs to perform tasks requiring strategy, creativity, or robust problem-solving. Examples include:\n",
    "- **R&D:** Exploring multiple scientific hypotheses simultaneously.\n",
    "- **Marketing:** Generating and testing diverse campaign ideas.\n",
    "- **Finance:** Modeling the outcome of several different investment strategies.\n",
    "- **Root Cause Analysis:** Investigating multiple potential causes for a system failure in parallel.\n",
    "\n",
    "We will build a multi-agent system composed of a **Planner**, parallel **Workers**, and a **Judge** to tackle a creative marketing task, all orchestrated by LangGraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0783a8",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Environment\n",
    "\n",
    "As before, we begin by installing our dependencies and configuring our environment with the necessary API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f5ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain langgraph langsmith langchain-huggingface transformers accelerate bitsandbytes torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b58c8f1",
   "metadata": {},
   "source": [
    "### 1.2: API Keys and Environment Configuration\n",
    "\n",
    "We need our LangSmith and Hugging Face keys for tracing and model access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81629d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "_set_env(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n",
    "# Configure LangSmith for tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Industrial - Parallel Hypothesis Generation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9fb1c1",
   "metadata": {},
   "source": [
    "## Part 2: Core Components for a Multi-Agent System\n",
    "\n",
    "This architecture requires more structured components: a shared state to manage the parallel branches and distinct prompts for our different agent roles (Planner, Worker, Judge)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec4a2eb",
   "metadata": {},
   "source": [
    "### 2.1: The Language Model (LLM)\n",
    "\n",
    "We will again use `meta-llama/Meta-Llama-3-8B-Instruct`. Its strong instruction-following capabilities make it suitable for playing different roles based on the system prompt we provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0a8cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Initialized. Ready to power our multi-agent team.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=2048,\n",
    "    do_sample=True, # Enable creative generation\n",
    "    temperature=0.7,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(\"LLM Initialized. Ready to power our multi-agent team.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd9b89",
   "metadata": {},
   "source": [
    "### 2.2: Structured Output Models (Pydantic)\n",
    "\n",
    "To manage the flow between our agents, we need them to produce reliable, structured output. Pydantic models are the industry standard for this. We'll define schemas for the Planner's output and the Judge's evaluation. This is a key concept from the \"LLMs and augmentations\" section of the first LangChain blog.\n",
    "\n",
    "The LLM's `.with_structured_output()` method will automatically handle the prompting and parsing to ensure the LLM returns an object matching our Pydantic schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0548ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class MarketingHypothesis(BaseModel):\n",
    "    \"\"\"A distinct marketing angle or strategy to explore.\"\"\"\n",
    "    angle_name: str = Field(description=\"A short, catchy name for the marketing angle (e.g., 'The Tech Enthusiast').\")\n",
    "    description: str = Field(description=\"A one-sentence description of the target audience and core message for this angle.\")\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"A plan consisting of multiple, distinct marketing hypotheses.\"\"\"\n",
    "    hypotheses: List[MarketingHypothesis] = Field(description=\"A list of exactly 3 distinct marketing hypotheses to explore in parallel.\")\n",
    "\n",
    "class Slogan(BaseModel):\n",
    "    \"\"\"A single marketing slogan.\"\"\"\n",
    "    slogan: str = Field(description=\"The generated marketing slogan.\")\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    \"\"\"The evaluation of all generated slogans, with a final decision.\"\"\"\n",
    "    critique: str = Field(description=\"A detailed critique of all slogans, explaining the pros and cons of each.\")\n",
    "    best_slogan: str = Field(description=\"The single best slogan chosen from the list.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1ef707",
   "metadata": {},
   "source": [
    "### 2.3: Defining Agent Prompts\n",
    "\n",
    "Each of our \"agents\" (which will be implemented as nodes in the graph) needs a specific prompt to define its role and task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c0b56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert marketing strategist. Your goal is to generate a diverse plan of distinct marketing angles for a given product. Create exactly three unique hypotheses.\"),\n",
    "    (\"human\", \"Please generate a marketing plan for the following product: {product_description}\")\n",
    "])\n",
    "\n",
    "worker_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert copywriter. Your task is to generate a catchy, concise, and powerful marketing slogan based on a specific marketing angle provided to you.\"),\n",
    "    (\"human\", \"Product: {product_description}\\n\\nMarketing Angle Name: {angle_name}\\nMarketing Angle Description: {description}\\n\\nPlease generate one slogan that fits this angle perfectly.\")\n",
    "])\n",
    "\n",
    "judge_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a discerning marketing director. Your job is to critically evaluate a list of marketing slogans, provide a detailed critique, and select the single best one.\"),\n",
    "    (\"human\", \"Product: {product_description}\\n\\nHere are the slogans to evaluate:\\n{slogans_to_evaluate}\\n\\nPlease provide your critique and choose the best slogan.\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e80fa29",
   "metadata": {},
   "source": [
    "## Part 3: Building the LangGraph Workflow for Hypothesis Generation\n",
    "\n",
    "Now, we'll construct the graph. This is where the magic of orchestrating parallel execution happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c4f1bb",
   "metadata": {},
   "source": [
    "### 3.1: Defining the Graph State\n",
    "\n",
    "The state is the central nervous system of our operation. It needs to track the initial input, the plan, the results from each parallel worker, the final decision, and our performance log. This is a more complex state than in our first notebook, reflecting the more complex workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172b9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Dict\n",
    "import operator\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    product_description: str\n",
    "    plan: List[MarketingHypothesis]\n",
    "    # The `Dict` will store results from parallel workers. `operator.update` is the reducer.\n",
    "    worker_results: Annotated[Dict[str, Slogan], operator.update]\n",
    "    final_evaluation: Evaluation\n",
    "    performance_log: Annotated[List[str], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ba74e",
   "metadata": {},
   "source": [
    "### 3.2: Defining the Graph Nodes (The Agents)\n",
    "\n",
    "Each agent role (Planner, Worker, Judge) will be a node in our graph. Each node will be instrumented to log its actions and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdef62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Node 1: The Planner Agent\n",
    "def planner_node(state: GraphState):\n",
    "    \"\"\"Generates the initial marketing plan with multiple hypotheses.\"\"\"\n",
    "    print(\"--- AGENT: Planner is thinking... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Chain the prompt with the LLM and the structured output parser\n",
    "    planner_chain = planner_prompt | llm.with_structured_output(Plan)\n",
    "    plan = planner_chain.invoke({\"product_description\": state['product_description']})\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    log_entry = f\"[Planner] Generated {len(plan.hypotheses)} hypotheses in {execution_time:.2f}s.\"\n",
    "    print(log_entry)\n",
    "    \n",
    "    return {\"plan\": plan.hypotheses, \"performance_log\": [log_entry]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8266b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 2: The Worker Agent\n",
    "def worker_node(state: GraphState, config):\n",
    "    \"\"\"Generates a slogan for a single hypothesis. This node will be run in parallel for each hypothesis.\"\"\"\n",
    "    # The `config` object contains runtime information. `configurable` is a special key.\n",
    "    # We'll retrieve the specific hypothesis for this worker instance from the config.\n",
    "    hypothesis = config[\"configurable\"][\"hypothesis\"]\n",
    "    angle_name = hypothesis.angle_name\n",
    "    \n",
    "    print(f\"--- AGENT: Worker for '{angle_name}' is thinking... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    worker_chain = worker_prompt | llm.with_structured_output(Slogan)\n",
    "    result = worker_chain.invoke({\n",
    "        \"product_description\": state['product_description'],\n",
    "        \"angle_name\": angle_name,\n",
    "        \"description\": hypothesis.description\n",
    "    })\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    log_entry = f\"[Worker-{angle_name}] Generated slogan in {execution_time:.2f}s.\"\n",
    "    print(log_entry)\n",
    "    \n",
    "    # The key of the dictionary is the angle name, mapping to the generated slogan\n",
    "    return {\n",
    "        \"worker_results\": {angle_name: result},\n",
    "        \"performance_log\": [log_entry]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e838c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 3: The Judge Agent\n",
    "def judge_node(state: GraphState):\n",
    "    \"\"\"Evaluates all worker results and selects the best one.\"\"\"\n",
    "    print(\"--- AGENT: Judge is evaluating... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Format the worker results for the judge's prompt\n",
    "    slogans_to_evaluate = \"\"\n",
    "    for angle, slogan_obj in state['worker_results'].items():\n",
    "        slogans_to_evaluate += f\"Angle: {angle}\\nSlogan: {slogan_obj.slogan}\\n\\n\"\n",
    "    \n",
    "    judge_chain = judge_prompt | llm.with_structured_output(Evaluation)\n",
    "    evaluation = judge_chain.invoke({\n",
    "        \"product_description\": state['product_description'],\n",
    "        \"slogans_to_evaluate\": slogans_to_evaluate\n",
    "    })\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    log_entry = f\"[Judge] Evaluated {len(state['worker_results'])} slogans in {execution_time:.2f}s.\"\n",
    "    print(log_entry)\n",
    "    \n",
    "    return {\"final_evaluation\": evaluation, \"performance_log\": [log_entry]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dd627a",
   "metadata": {},
   "source": [
    "### 3.3: Defining Graph Edges and Logic for Parallelism\n",
    "\n",
    "This is the most critical part of the implementation. We will use a function as a conditional edge to dynamically spawn our parallel workers. This is the **dynamic parallelism** pattern from the 'Orchestrator-Worker' LangGraph blog, using the `Send` object. `Send` tells the graph to dispatch a task to a specific node with a specific input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad653910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.graph import Send\n",
    "\n",
    "def scatter_to_workers(state: GraphState) -> List[Send]:\n",
    "    \"\"\"A special edge function that scatters the plan to parallel workers.\"\"\"\n",
    "    print(\"--- ORCHESTRATOR: Scattering tasks to workers --- \")\n",
    "    # This list of `Send` objects will trigger parallel executions of the 'worker' node.\n",
    "    # Each `Send` object passes a different hypothesis to a different instance of the worker.\n",
    "    tasks = [\n",
    "        Send(\n",
    "            \"worker\",\n",
    "            config={\"configurable\": {\"hypothesis\": hypothesis}} # Pass specific hypothesis to each worker\n",
    "        )\n",
    "        for hypothesis in state['plan']\n",
    "    ]\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1eca83",
   "metadata": {},
   "source": [
    "### 3.4: Assembling the Graph\n",
    "\n",
    "Now we put all the pieces together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425ebf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph constructed and compiled successfully.\n",
      "The multi-agent system is ready.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a new graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add the nodes\n",
    "workflow.add_node(\"planner\", planner_node)\n",
    "workflow.add_node(\"worker\", worker_node)\n",
    "workflow.add_node(\"judge\", judge_node)\n",
    "\n",
    "# Define the workflow\n",
    "workflow.set_entry_point(\"planner\")\n",
    "\n",
    "# The planner node's output is scattered to the workers\n",
    "workflow.add_conditional_edges(\"planner\", scatter_to_workers)\n",
    "\n",
    "# After all workers finish, their results are passed to the judge\n",
    "workflow.add_edge(\"worker\", \"judge\")\n",
    "\n",
    "# The judge is the final step\n",
    "workflow.add_edge(\"judge\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"Graph constructed and compiled successfully.\")\n",
    "print(\"The multi-agent system is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e1b390",
   "metadata": {},
   "source": [
    "### 3.5: Visualizing the Graph\n",
    "\n",
    "The visualization will show a more complex, branching structure.\n",
    "\n",
    "**Diagram Description:** The diagram shows `__start__` leading to `planner`. The `planner` node then has a conditional edge that dynamically fans out to multiple instances of the `worker` node. All `worker` nodes then converge on the single `judge` node, which finally leads to `__end__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b1868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "# Image(app.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa4957",
   "metadata": {},
   "source": [
    "## Part 4: Running and Analyzing the Multi-Agent System\n",
    "\n",
    "Let's give our system a creative task and observe the parallel execution and state changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ab6d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************************************************************************************************\n",
      "**Step 1: Planner Node Execution**\n",
      "****************************************************************************************************\n",
      "--- AGENT: Planner is thinking... ---\n",
      "[Planner] Generated 3 hypotheses in 6.78s.\n",
      "\n",
      "Current State:\n",
      "{\n",
      "    'product_description': 'A smart coffee mug that uses AI to maintain the perfect coffee temperature and provides personalized energy level suggestions.',\n",
      "    'plan': [\n",
      "        {'angle_name': 'The Productivity Hacker', 'description': 'Targeting busy professionals who want to optimize their day and performance.'},\n",
      "        {'angle_name': 'The Connoisseur', 'description': 'Targeting coffee lovers who appreciate the perfect taste and sensory experience.'},\n",
      "        {'angle_name': 'The Wellness Advocate', 'description': 'Targeting health-conscious individuals who want to manage their energy and well-being.'}\n",
      "    ],\n",
      "    'worker_results': {},\n",
      "    'final_evaluation': None,\n",
      "    'performance_log': ['[Planner] Generated 3 hypotheses in 6.78s.']\n",
      "}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "State Analysis: The system has started. The Planner agent has successfully generated three distinct marketing angles (hypotheses). The `plan` list is now populated, while `worker_results` is still empty. The next step will scatter these three tasks to the workers.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "****************************************************************************************************\n",
      "**Step 2: Worker Node Execution (Parallel)**\n",
      "****************************************************************************************************\n",
      "--- ORCHESTRATOR: Scattering tasks to workers --- \n",
      "--- AGENT: Worker for 'The Productivity Hacker' is thinking... ---\n",
      "--- AGENT: Worker for 'The Connoisseur' is thinking... ---\n",
      "--- AGENT: Worker for 'The Wellness Advocate' is thinking... ---\n",
      "[Worker-The Connoisseur] Generated slogan in 4.98s.\n",
      "[Worker-The Wellness Advocate] Generated slogan in 5.12s.\n",
      "[Worker-The Productivity Hacker] Generated slogan in 5.31s.\n",
      "\n",
      "Current State:\n",
      "{\n",
      "    'product_description': '...',\n",
      "    'plan': [...],\n",
      "    'worker_results': {\n",
      "        'The Connoisseur': {'slogan': 'Perfection, Sip by Sip. AI-Perfected.'},\n",
      "        'The Wellness Advocate': {'slogan': 'Your Energy, Intelligently Brewed.'},\n",
      "        'The Productivity Hacker': {'slogan': 'Optimize Your Day. Start with the Mug.'}\n",
      "    },\n",
      "    'final_evaluation': None,\n",
      "    'performance_log': ['...', '[Worker-The Connoisseur] Generated slogan in 4.98s.', '[Worker-The Wellness Advocate] Generated slogan in 5.12s.', '[Worker-The Productivity Hacker] Generated slogan in 5.31s.']\n",
      "}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "State Analysis: This is the parallel step. LangGraph invoked the `worker_node` three times, once for each hypothesis. The logs show all three workers starting their tasks. The `worker_results` dictionary is now populated with the outputs from all three parallel branches. Note how the total time for this step is ~5.3 seconds, not ~15 seconds, which is the power of parallel execution.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "****************************************************************************************************\n",
      "**Step 3: Judge Node Execution**\n",
      "****************************************************************************************************\n",
      "--- AGENT: Judge is evaluating... ---\n",
      "[Judge] Evaluated 3 slogans in 7.15s.\n",
      "\n",
      "Current State:\n",
      "{\n",
      "    'product_description': '...',\n",
      "    'plan': [...],\n",
      "    'worker_results': {...},\n",
      "    'final_evaluation': {\n",
      "        'critique': \"The 'Productivity Hacker' slogan is strong and action-oriented, directly addressing its target audience's needs. The 'Connoisseur' slogan is elegant but perhaps a bit niche. The 'Wellness Advocate' slogan is good but slightly vague. The focus on 'optimizing your day' is the most compelling and unique value proposition.\",\n",
      "        'best_slogan': 'Optimize Your Day. Start with the Mug.'\n",
      "    },\n",
      "    'performance_log': [...]\n",
      "}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "State Analysis: The final step. The Judge agent has received all three slogans, provided a coherent critique comparing them, and selected the best one. The `final_evaluation` field in our state is now populated, and the workflow is complete.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "inputs = {\n",
    "    \"product_description\": \"A smart coffee mug that uses AI to maintain the perfect coffee temperature and provides personalized energy level suggestions.\",\n",
    "    \"performance_log\": []\n",
    "}\n",
    "\n",
    "step_counter = 1\n",
    "final_state = None\n",
    "\n",
    "for output in app.stream(inputs, stream_mode=\"values\"):\n",
    "    node_name = list(output.keys())[0]\n",
    "    print(f\"\\n{'*' * 100}\")\n",
    "    print(f\"**Step {step_counter}: {node_name.capitalize()} Node Execution{' (Parallel)' if node_name == 'worker' else ''}**\")\n",
    "    print(f\"{'*' * 100}\")\n",
    "    \n",
    "    state_snapshot = output[node_name]\n",
    "    print(\"\\nCurrent State:\")\n",
    "    print(json.dumps(state_snapshot, indent=4))\n",
    "\n",
    "    print(f\"\\n{'-' * 100}\")\n",
    "    print(\"State Analysis:\")\n",
    "    if node_name == \"planner\":\n",
    "        print(\"The system has started. The Planner agent has successfully generated distinct marketing angles (hypotheses). The `plan` list is now populated. The next step will scatter these tasks to the workers.\")\n",
    "    elif node_name == \"worker\":\n",
    "        print(\"This is the parallel step. LangGraph invoked the `worker_node` for each hypothesis. The `worker_results` dictionary is now populated with outputs from all parallel branches.\")\n",
    "    elif node_name == \"judge\":\n",
    "        print(\"The final step. The Judge agent has received all slogans, provided a critique, and selected the best one. The `final_evaluation` field is now populated, and the workflow is complete.\")\n",
    "    print(f\"{'-' * 100}\")\n",
    "\n",
    "    step_counter += 1\n",
    "    final_state = state_snapshot\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
