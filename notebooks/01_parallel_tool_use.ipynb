{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1 (Industrial Edition): High-Performance Parallel Tool Use\n",
    "\n",
    "## Introduction: From Theory to Production-Grade Efficiency\n",
    "\n",
    "This notebook elevates the concept of **Parallel Tool Use** from a simple demonstration to a production-oriented implementation. We are moving beyond mock tools to interact with **real-world, live APIs**. Our focus will be on building a robust, observable, and high-performance agent that mirrors the standards of an industrial application.\n",
    "\n",
    "### The Core Problem in Large-Scale Systems\n",
    "\n",
    "Large-scale agentic systems are not just about intelligence; they are about performance. A system that takes 10 seconds to answer a user's query is functionally useless for interactive applications. The primary bottleneck is almost always I/O latency\u2014waiting for networks, databases, and external APIs. This notebook tackles this problem head-on by implementing a parallel tool-calling architecture.\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "1.  **Real-World Tool Integration:** How to wrap live APIs (`yfinance` for stocks, `Tavily` for news) into LangChain tools.\n",
    "2.  **Intensive Instrumentation:** How to measure and log the performance (execution time, statistics) of each component in your agentic workflow.\n",
    "3.  **Verbose State Tracking:** How to inspect the `State` of your graph at every step to deeply understand the data flow and decision-making process.\n",
    "4.  **Parallel Execution Analysis:** How to quantify the performance gains of a parallel design compared to a sequential one.\n",
    "\n",
    "This implementation directly applies the concepts from the LangGraph blogs, using a `StateGraph` to manage a conversational `MessagesState` and conditional edges to route logic between a powerful LLM and a tool execution engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Environment\n",
    "\n",
    "We begin by installing the required libraries. This time, we include `yfinance` for stock data and `tavily-python` for the search API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ca4ae",
   "metadata": {},
   "source": [
    "#### Create a venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfe084e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the virtual environment\n",
    "import os\n",
    "!uv venv .venv\n",
    "\n",
    "# Activate the virtual environment for the shell commands in this notebook\n",
    "os.environ['VIRTUAL_ENV'] = os.path.abspath(\".venv\")\n",
    "os.environ['PATH'] = os.path.abspath(\".venv/bin\") + \":\" + os.environ['PATH']\n",
    "\n",
    "# Install packages into the created virtual environment\n",
    "!uv pip install -U langchain langgraph langsmith langchain-huggingface transformers accelerate bitsandbytes torch yfinance tavily-python python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9f8ab3",
   "metadata": {},
   "source": [
    "#### Activate the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43134089",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f9666e",
   "metadata": {},
   "source": [
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3344b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!uv pip install --python {sys.executable} -U langchain langgraph langsmith langchain-community langchain-tavily langchain-huggingface transformers accelerate bitsandbytes torch yfinance tavily-python ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1926889e",
   "metadata": {},
   "source": [
    "### 1.2: API Keys and Environment Configuration\n",
    "\n",
    "We need three keys for this notebook:\n",
    "- **LangSmith API Key:** For tracing and debugging. Absolutely essential for production systems.\n",
    "- **Hugging Face Token:** To download the Llama 3 model.\n",
    "- **Tavily API Key:** For our real-world news and search tool.\n",
    "\n",
    "Get your keys here:\n",
    "- LangSmith: [smith.langchain.com](https://smith.langchain.com)\n",
    "- Hugging Face: [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "- Tavily: [app.tavily.com](https://app.tavily.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c0c1635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure LangSmith for tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Industrial - Parallel Tool Use\"\n",
    "\n",
    "# Verify API keys\n",
    "print(\"API keys loaded:\")\n",
    "print(\"OPENAI_API_KEY:\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "print(\"HUGGING_FACE_HUB_TOKEN:\", os.getenv(\"HUGGING_FACE_HUB_TOKEN\"))\n",
    "print(\"LANGCHAIN_API_KEY:\", os.getenv(\"LANGCHAIN_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Defining Production-Grade Components\n",
    "\n",
    "Here, we define our LLM and integrate our live API tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: The Language Model (LLM)\n",
    "\n",
    "We'll use `meta-llama/Meta-Llama-3-8B-Instruct` as our agent's brain. Loading with 4-bit quantization via `bitsandbytes` (`load_in_4bit=True`) is a crucial technique for running large models on consumer hardware. `device_map=\"auto\"` intelligently distributes the model across available GPUs and CPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1531bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e50e3525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Loading model on {device}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16 if device == \"mps\" else torch.float32,\n",
    "    device_map=device,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=2048,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "llm_pipeline = HuggingFacePipeline(pipeline=pipe)\n",
    "llm = ChatHuggingFace(llm=llm_pipeline)\n",
    "\n",
    "print(f\"\u2705 LLM loaded: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "95b9873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import yfinance as yf\n",
    "import os\n",
    "\n",
    "# Tool 1: Stock Price\n",
    "@tool\n",
    "def get_stock_price(symbol: str) -> float:\n",
    "    \"\"\"Get the current stock price for a given stock symbol using Yahoo Finance.\"\"\"\n",
    "    print(f\"--- [Tool Call] Executing get_stock_price for symbol: {symbol} ---\")\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    price = ticker.info.get('regularMarketPrice', ticker.info.get('currentPrice'))\n",
    "    if price is None:\n",
    "        return f\"Could not find price for symbol {symbol}\"\n",
    "    return price\n",
    "\n",
    "# Tool 2: Company News\n",
    "tavily_search = TavilySearchResults(\n",
    "    max_results=5,\n",
    "    api_key=os.getenv(\"TAVILY_API_KEY\")\n",
    ")\n",
    "\n",
    "@tool\n",
    "def get_recent_company_news(company_name: str) -> list:\n",
    "    \"\"\"Get recent news articles and summaries for a given company name using the Tavily search engine.\"\"\"\n",
    "    print(f\"--- [Tool Call] Executing get_recent_company_news for: {company_name} ---\")\n",
    "    query = f\"latest news about {company_name}\"\n",
    "    return tavily_search.invoke(query)\n",
    "\n",
    "# CREATE THE TOOLS LIST\n",
    "tools = [get_stock_price, get_recent_company_news]\n",
    "\n",
    "print(f\"\u2705 Tools defined: {[tool.name for tool in tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: The Real-World Tools\n",
    "\n",
    "This is where our implementation becomes real. We'll define two tools that call live APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1: Stock Price Tool (`yfinance`)\n",
    "\n",
    "We use the `yfinance` library to get real-time stock data. The `@tool` decorator's docstring is critical: it's the instruction manual the LLM uses to understand how to use the tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the `get_stock_price` tool to see its live output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stock_price.invoke({\"symbol\": \"NVDA\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2: Company News Tool (`Tavily`)\n",
    "\n",
    "We'll use the `TavilySearchResults` tool, a powerful search API optimized for LLMs. We'll wrap it in our own `@tool` function to provide a more specific docstring, guiding the LLM to use it specifically for finding recent news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the news tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recent_company_news.invoke({\"company_name\": \"NVIDIA\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Binding Tools and Creating the Executor\n",
    "\n",
    "We now collect our tools and bind them to the LLM. The `ToolNode` is a LangGraph utility that efficiently calls the requested tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "52853703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Create tool node\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(\"\u2705 llm_with_tools created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5d1d133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what's defined\n",
    "print(\"llm defined:\", 'llm' in dir())\n",
    "print(\"tools defined:\", 'tools' in dir())\n",
    "print(\"llm_with_tools defined:\", 'llm_with_tools' in dir())\n",
    "print(\"app defined:\", 'app' in dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"State schema for the agent graph.\"\"\"\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    performance_log: Annotated[List[str], operator.add]\n",
    "\n",
    "print(\"\u2705 AgentState defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "14ba72f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def call_model(state: AgentState):\n",
    "    \"\"\"The agent node: calls the LLM.\"\"\"\n",
    "    print(\"--- AGENT: Invoking LLM --- \")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    messages = state['messages']\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    log_entry = f\"[AGENT] LLM call took {execution_time:.2f} seconds.\"\n",
    "    print(log_entry)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"performance_log\": [log_entry]\n",
    "    }\n",
    "\n",
    "print(\"\u2705 call_model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3bebe280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"Determine whether to continue to tools or end.\"\"\"\n",
    "    last_message = state['messages'][-1]\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "print(\"\u2705 should_continue defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "83477ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Create tool node\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# Define the graph\n",
    "workflow = StateGraph(AgentState)  # \u2705 Now AgentState exists!\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", END: END})\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"\u2705 Graph constructed and compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building an Instrumented LangGraph Workflow\n",
    "\n",
    "Now we define the graph itself, adding detailed instrumentation to track performance and state changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Defining an Enhanced Graph State\n",
    "\n",
    "We will expand our state beyond just messages. To properly instrument our agent, we will also track performance metrics. We'll use a `TypedDict` to define a structured state. `Annotated` allows us to add a reducer function (`operator.add`) to `execution_time` so that it accumulates across steps, as explained in the 'Graph API Overview' blog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The history of messages\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    # A list to store performance data for each step\n",
    "    performance_log: Annotated[List[str], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Defining Instrumented Graph Nodes\n",
    "\n",
    "Our nodes will now do more than just their core task; they will also measure their own execution time and log their actions to the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def call_model(state: AgentState):\n",
    "    \"\"\"The agent node: calls the LLM, measures performance, and logs the result.\"\"\"\n",
    "    print(\"--- AGENT: Invoking LLM --- \")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    messages = state['messages']\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    # Log performance\n",
    "    log_entry = f\"[AGENT] LLM call took {execution_time:.2f} seconds.\"\n",
    "    print(log_entry)\n",
    "    \n",
    "    # The response is a new message to be added to the list\n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"performance_log\": [log_entry]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "def call_tool(state: AgentState):\n",
    "    \"\"\"The tool node: executes tools, measures performance, and logs the results.\"\"\"\n",
    "    print(\"--- TOOLS: Executing tool calls --- \")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    last_message = state['messages'][-1]\n",
    "    tool_invocations = last_message.tool_calls\n",
    "    \n",
    "    # The ToolExecutor can batch-execute tool calls.\n",
    "    # For sync tools, this is sequential. For async tools, it would be parallel.\n",
    "    responses = tool_executor.batch(tool_invocations)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    # Log performance\n",
    "    log_entry = f\"[TOOLS] Executed {len(tool_invocations)} tools in {execution_time:.2f} seconds.\"\n",
    "    print(log_entry)\n",
    "    \n",
    "    # Format responses as ToolMessages\n",
    "    tool_messages = [\n",
    "        ToolMessage(content=str(response), tool_call_id=call['id'])\n",
    "        for call, response in zip(tool_invocations, responses)\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"messages\": tool_messages,\n",
    "        \"performance_log\": [log_entry]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Defining the Graph Edges and Assembling the Graph\n",
    "\n",
    "The logic for routing remains the same as our basic example. This conditional branching is a core pattern in LangGraph. After defining the edge, we assemble the full graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    last_message = state['messages'][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "# Define the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", call_tool)\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", END: END})\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile the graph into a runnable app\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"Graph constructed and compiled successfully.\")\n",
    "print(\"The agent is ready to be run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4: Visualizing the Graph\n",
    "\n",
    "Visualizing the graph helps confirm our logic. The structure is a simple loop: the agent thinks, optionally uses tools, and then thinks again with the new information.\n",
    "\n",
    "**Diagram Description:** The diagram shows `__start__` connected to `agent`. The `agent` node has two conditional outputs: one to `__end__` and one to `tools`. The `tools` node has a single, unconditional edge leading back to `agent`, forming the agent's core reasoning and action loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6648a62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate interactive HTML visualization\n",
    "html_content = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>LangGraph Visualization</title>\n",
    "    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/cytoscape/3.26.0/cytoscape.min.js\"></script>\n",
    "    <style>\n",
    "        #cy { width: 100%; height: 500px; border: 2px solid #ccc; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h2>\ud83d\udd04 Agent Graph Visualization</h2>\n",
    "    <div id=\"cy\"></div>\n",
    "    <script>\n",
    "        var cy = cytoscape({\n",
    "            container: document.getElementById('cy'),\n",
    "            elements: [\n",
    "                { data: { id: 'start', label: '__start__' } },\n",
    "                { data: { id: 'agent', label: 'agent' } },\n",
    "                { data: { id: 'tools', label: 'tools' } },\n",
    "                { data: { id: 'end', label: '__end__' } },\n",
    "                { data: { source: 'start', target: 'agent' } },\n",
    "                { data: { source: 'agent', target: 'tools', label: 'has_tool_calls' } },\n",
    "                { data: { source: 'agent', target: 'end', label: 'no_tool_calls' } },\n",
    "                { data: { source: 'tools', target: 'agent' } }\n",
    "            ],\n",
    "            style: [\n",
    "                {\n",
    "                    selector: 'node',\n",
    "                    style: {\n",
    "                        'background-color': '#4A90E2',\n",
    "                        'label': 'data(label)',\n",
    "                        'color': '#fff',\n",
    "                        'text-valign': 'center',\n",
    "                        'width': 60,\n",
    "                        'height': 60\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    selector: 'edge',\n",
    "                    style: {\n",
    "                        'width': 2,\n",
    "                        'line-color': '#666',\n",
    "                        'target-arrow-color': '#666',\n",
    "                        'target-arrow-shape': 'triangle',\n",
    "                        'curve-style': 'bezier',\n",
    "                        'label': 'data(label)',\n",
    "                        'font-size': 10\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            layout: { name: 'breadthfirst', directed: true }\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open('graph_viz.html', 'w') as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "print(\"\u2705 Interactive visualization saved to: graph_viz.html\")\n",
    "print(\"Open it in your browser!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Running and Analyzing the Instrumented Agent\n",
    "\n",
    "Now we execute the agent. We'll stream the full state at each step (`stream_mode='values'`) to see exactly how `messages` and `performance_log` evolve. The user query is designed to trigger both tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [HumanMessage(content=\"What is the current stock price of NVIDIA (NVDA) and what is the latest news about the company?\")],\n",
    "    \"performance_log\": []\n",
    "}\n",
    "\n",
    "step_counter = 1\n",
    "final_state = None\n",
    "\n",
    "for output in app.stream(inputs, stream_mode=\"values\"):\n",
    "    print(f\"\\n{'*' * 100}\")\n",
    "    print(f\"**Step {step_counter}**\")\n",
    "    print(f\"{'*' * 100}\")\n",
    "    \n",
    "    # Print messages\n",
    "    if 'messages' in output:\n",
    "        print(f\"\\n\ud83d\udcac Messages: {len(output['messages'])} total\")\n",
    "        last_msg = output['messages'][-1]\n",
    "        msg_type = type(last_msg).__name__\n",
    "        print(f\"   Last message type: {msg_type}\")\n",
    "        \n",
    "        # Print content\n",
    "        if hasattr(last_msg, 'content'):\n",
    "            content = str(last_msg.content)[:200]\n",
    "            print(f\"   Content: {content}...\")\n",
    "        \n",
    "        # Print tool calls if present\n",
    "        if hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:\n",
    "            print(f\"   Tool calls: {len(last_msg.tool_calls)}\")\n",
    "            for tc in last_msg.tool_calls:\n",
    "                print(f\"      \u2022 {tc.get('name', 'unknown')}\")\n",
    "    \n",
    "    # Print performance log\n",
    "    if 'performance_log' in output:\n",
    "        print(f\"\\n\u23f1\ufe0f  Performance:\")\n",
    "        for log in output['performance_log']:\n",
    "            print(f\"   {log}\")\n",
    "    \n",
    "    print(f\"\\n{'-' * 100}\")\n",
    "    \n",
    "    step_counter += 1\n",
    "    final_state = output\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"EXECUTION COMPLETE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Print final response\n",
    "if final_state and 'messages' in final_state:\n",
    "    last_msg = final_state['messages'][-1]\n",
    "    if hasattr(last_msg, 'content'):\n",
    "        print(\"\\n\ud83d\udccb Final Response:\")\n",
    "        print(last_msg.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}