{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8531e089",
   "metadata": {},
   "source": [
    "# Notebook 11 (Industrial Edition): Sharded & Scattered Retrieval for Large-Scale RAG\n",
    "\n",
    "## Introduction: The Infrastructure for Petabyte-Scale Knowledge\n",
    "\n",
    "This notebook tackles a fundamental challenge of production RAG: how to maintain low-latency, high-accuracy retrieval when your knowledge base grows to millions or billions of documents. The solution is an infrastructure pattern known as **Sharded & Scattered Retrieval**.\n",
    "\n",
    "### The Core Concept: Divide and Conquer\n",
    "\n",
    "Instead of a single, massive vector store that becomes slow and unwieldy, we partition (shard) our knowledge base into multiple smaller, independent vector stores. These shards can be organized by topic, date, data source, or any other logical division. When a user query arrives, a central orchestrator \"scatters\" the query to all shards, which perform their searches in parallel. The results are then \"gathered\" and re-ranked to find the globally best documents.\n",
    "\n",
    "### Role in a Large-Scale System: Enabling Low-Latency Knowledge Access Across Enterprise-Scale Data\n",
    "\n",
    "This is the *only* viable architecture for building a true enterprise-grade search or RAG system. It is the foundation for:\n",
    "- **Scalability:** The system can scale horizontally to accommodate a virtually infinite amount of data by simply adding more shards.\n",
    "- **Performance:** Retrieval latency remains low and constant because searches are performed on smaller, faster indexes.\n",
    "- **Accuracy & Organization:** Sharding allows for the creation of specialized knowledge domains. A search for a technical term can be prioritized in the engineering shard, leading to more relevant results and higher accuracy.\n",
    "\n",
    "We will build a simulated two-shard RAG system (Engineering vs. Marketing) and compare it to a monolithic system to demonstrate the concrete benefits in both **latency** and **answer quality**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5596ff02",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Environment\n",
    "\n",
    "We'll need our standard libraries plus `langchain-community` for vector stores and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c673e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain langgraph langsmith langchain-huggingface transformers accelerate bitsandbytes torch langchain-community sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd86e80e",
   "metadata": {},
   "source": [
    "### 1.2: API Keys and Environment Configuration\n",
    "\n",
    "We will need our LangSmith and Hugging Face keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "_set_env(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n",
    "# Configure LangSmith for tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Industrial - RAG Sharded Retrieval\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377d07f9",
   "metadata": {},
   "source": [
    "## Part 2: Components for the Sharded RAG System\n",
    "\n",
    "We'll need an LLM and, crucially, we will build multiple, distinct knowledge base shards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20016f0e",
   "metadata": {},
   "source": [
    "### 2.1: The Language Model (LLM)\n",
    "\n",
    "We will use `meta-llama/Meta-Llama-3-8B-Instruct` for our generator agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c2e3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Initialized. Ready to power our RAG system.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", load_in_4bit=True)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=1024, do_sample=False)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(\"LLM Initialized. Ready to power our RAG system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced1b10",
   "metadata": {},
   "source": [
    "### 2.2: Creating the Knowledge Base Shards\n",
    "\n",
    "We will create two separate vector stores to simulate our shards. Each will contain domain-specific information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b162a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Base shards created: Engineering KB (3 docs), Marketing KB (3 docs).\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Engineering KB Documents\n",
    "eng_docs = [\n",
    "    Document(page_content=\"The QuantumLeap V3 processor utilizes a 3nm process node and features a dedicated AI accelerator core with 128 tensor units. API endpoint `/api/v3/status` provides real-time thermal throttling data.\", metadata={\"source\": \"eng-kb\"}),\n",
    "    Document(page_content=\"Firmware update v2.1 for the Aura Smart Ring optimizes the photoplethysmography (PPG) sensor algorithm for more accurate sleep stage detection. The update is deployed via the mobile app.\", metadata={\"source\": \"eng-kb\"}),\n",
    "    Document(page_content=\"The Smart Mug's heating element is a nickel-chromium coil controlled by a PID controller. It maintains temperature within +/- 1 degree Celsius. Battery polling is done via the `getBattery` function.\", metadata={\"source\": \"eng-kb\"})\n",
    "]\n",
    "\n",
    "# Marketing KB Documents\n",
    "mkt_docs = [\n",
    "    Document(page_content=\"Press Release: Unveiling the QuantumLeap V3, the AI processor that redefines speed. 'It's a game-changer for creative professionals,' says CEO Jane Doe. Available Q4.\", metadata={\"source\": \"mkt-kb\"}),\n",
    "    Document(page_content=\"Product Page: The Aura Smart Ring is your personal wellness companion. Crafted from aerospace-grade titanium, it empowers you to unlock your full potential by understanding your body's signals.\", metadata={\"source\": \"mkt-kb\"}),\n",
    "    Document(page_content=\"Blog Post: 'Five Ways Our Smart Mug Supercharges Your Morning Routine.' The perfect temperature, from the first sip to the last, means your coffee is always perfect.\", metadata={\"source\": \"mkt-kb\"})\n",
    "]\n",
    "\n",
    "# Create embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create the two vector store shards\n",
    "eng_vectorstore = FAISS.from_documents(eng_docs, embedding=embeddings)\n",
    "mkt_vectorstore = FAISS.from_documents(mkt_docs, embedding=embeddings)\n",
    "\n",
    "eng_retriever = eng_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "mkt_retriever = mkt_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "print(f\"Knowledge Base shards created: Engineering KB ({len(eng_docs)} docs), Marketing KB ({len(mkt_docs)} docs).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81d8b2b",
   "metadata": {},
   "source": [
    "## Part 3: The Baseline - A Monolithic RAG System\n",
    "\n",
    "To establish a baseline, we'll first create a traditional RAG system with a single, combined knowledge base. We will add a simulated latency to its retrieval step to mimic searching a much larger index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51f8e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# 1. Create the monolithic vector store\n",
    "all_docs = eng_docs + mkt_docs\n",
    "monolithic_vectorstore = FAISS.from_documents(all_docs, embedding=embeddings)\n",
    "monolithic_retriever = monolithic_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# 2. Simulate the increased latency of a large index\n",
    "def slow_retrieval(query):\n",
    "    print(\"--- [Monolithic Retriever] Searching large index... (simulating high latency) ---\")\n",
    "    time.sleep(2.5) # Simulate latency\n",
    "    return monolithic_retriever.invoke(query)\n",
    "\n",
    "slow_monolithic_retriever = RunnableLambda(slow_retrieval)\n",
    "\n",
    "# 3. Create the monolithic RAG chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "generator_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert technical and marketing support agent. Answer the user's question based *only* on the provided context.\\n\\nContext:\\n{context}\"),\n",
    "    (\"human\", \"Question: {question}\")\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(f\"[Source: {doc.metadata.get('source', 'N/A')}] {doc.page_content}\" for doc in docs)\n",
    "\n",
    "monolithic_rag_chain = (\n",
    "    {\"context\": slow_monolithic_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | generator_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8c7145",
   "metadata": {},
   "source": [
    "## Part 4: Building the Sharded RAG Graph\n",
    "\n",
    "Now, let's build the superior, sharded system. The core of this system is a node that scatters the query to our two shards in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cd0769",
   "metadata": {},
   "source": [
    "### 4.1: Defining the Graph State and Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431520d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "class ShardedRAGState(TypedDict):\n",
    "    question: str\n",
    "    retrieved_docs: List[Document]\n",
    "    final_answer: str\n",
    "\n",
    "# Node 1: Parallel Retrieval (Scatter-Gather)\n",
    "def parallel_retrieval_node(state: ShardedRAGState):\n",
    "    \"\"\"Scatters the query to all shards and gathers the results.\"\"\"\n",
    "    print(\"--- [Meta-Retriever] Scattering query to Engineering and Marketing shards in parallel... ---\")\n",
    "    \n",
    "    # We'll use a ThreadPool to run retrievals concurrently\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        # P_retrieval function to add a delay to each shard search\n",
    "        def p_retrieval(retriever):\n",
    "            time.sleep(0.5) # Simulate network hop and smaller index search time\n",
    "            return retriever.invoke(state['question'])\n",
    "        \n",
    "        futures = [executor.submit(p_retrieval, retriever) for retriever in [eng_retriever, mkt_retriever]]\n",
    "        \n",
    "        all_docs = []\n",
    "        for future in futures:\n",
    "            all_docs.extend(future.result())\n",
    "    \n",
    "    # In a real system, you'd add a re-ranking step here. For now, we'll just deduplicate.\n",
    "    unique_docs = list({doc.page_content: doc for doc in all_docs}.values())\n",
    "    print(f\"--- [Meta-Retriever] Gathered {len(unique_docs)} unique documents from 2 shards. ---\")\n",
    "    return {\"retrieved_docs\": unique_docs}\n",
    "\n",
    "# Node 2: Generation Node (same as before)\n",
    "def generation_node(state: ShardedRAGState):\n",
    "    \"\"\"Synthesizes the final answer from the gathered documents.\"\"\"\n",
    "    print(\"--- [Generator] Synthesizing final answer... ---\")\n",
    "    context = format_docs(state['retrieved_docs'])\n",
    "    answer = (\n",
    "        generator_prompt \n",
    "        | llm \n",
    "        | StrOutputParser()\n",
    "    ).invoke({\"context\": context, \"question\": state['question']})\n",
    "    return {\"final_answer\": answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9191d8",
   "metadata": {},
   "source": [
    "### 4.2: Assembling the Sharded Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5435ec5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharded RAG graph compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(ShardedRAGState)\n",
    "workflow.add_node(\"parallel_retrieval\", parallel_retrieval_node)\n",
    "workflow.add_node(\"generate_answer\", generation_node)\n",
    "\n",
    "workflow.set_entry_point(\"parallel_retrieval\")\n",
    "workflow.add_edge(\"parallel_retrieval\", \"generate_answer\")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "\n",
    "sharded_rag_app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44cb533",
   "metadata": {},
   "source": [
    "## Part 5: Head-to-Head Comparison\n",
    "\n",
    "Now we will ask both systems a question that requires information from *both* the engineering and marketing knowledge bases to be answered completely and accurately. The monolithic system may struggle to find the less-dominant but still relevant context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24713bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query has strong marketing keywords ('game-changer', 'creative professionals')\n",
    "# but also a specific technical question ('API status endpoint').\n",
    "user_query = \"I heard the new QuantumLeap V3 is a 'game-changer for creative professionals'. Can you tell me more about it, and is there an API endpoint to check its status?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eeabe4",
   "metadata": {},
   "source": [
    "### 5.1: Running the Monolithic RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08595ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [MONOLITHIC RAG] Starting run... ---\n",
      "--- [Monolithic Retriever] Searching large index... (simulating high latency) ---\n",
      "\n",
      "=============================================================\n",
      "               MONOLITHIC RAG SYSTEM OUTPUT\n",
      "=============================================================\n",
      "\n",
      "Retrieved Context:\n",
      "[Source: mkt-kb] Press Release: Unveiling the QuantumLeap V3, the AI processor that redefines speed. 'It's a game-changer for creative professionals,' says CEO Jane Doe. Available Q4.\n",
      "[Source: eng-kb] The QuantumLeap V3 processor utilizes a 3nm process node and features a dedicated AI accelerator core with 128 tensor units. API endpoint `/api/v3/status` provides real-time thermal throttling data.\n",
      "[Source: mkt-kb] Product Page: The Aura Smart Ring is your personal wellness companion. Crafted from aerospace-grade titanium, it empowers you to unlock your full potential by understanding your body's signals.\n",
      "\n",
      "Final Answer:\n",
      "The QuantumLeap V3 is an AI processor described as a 'game-changer for creative professionals' according to CEO Jane Doe, and it will be available in Q4. It uses a 3nm process and has an API endpoint at `/api/v3/status` for checking thermal data.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- [MONOLITHIC RAG] Starting run... ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# We'll capture the context to inspect it\n",
    "retrieved_context_mono = \"\"\n",
    "def capture_context_mono(docs):\n",
    "    global retrieved_context_mono\n",
    "    retrieved_context_mono = format_docs(docs)\n",
    "    return retrieved_context_mono\n",
    "\n",
    "monolithic_rag_chain_instrumented = (\n",
    "    {\"context\": slow_monolithic_retriever | capture_context_mono, \"question\": RunnablePassthrough()}\n",
    "    | generator_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "monolithic_answer = monolithic_rag_chain_instrumented.invoke(user_query)\n",
    "monolithic_time = time.time() - start_time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"               MONOLITHIC RAG SYSTEM OUTPUT\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "print(\"Retrieved Context:\")\n",
    "print(retrieved_context_mono + \"\\n\")\n",
    "print(\"Final Answer:\")\n",
    "print(monolithic_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe3216",
   "metadata": {},
   "source": [
    "### 5.2: Running the Sharded RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331a7861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [SHARDED RAG] Starting run... ---\n",
      "--- [Meta-Retriever] Scattering query to Engineering and Marketing shards in parallel... ---\n",
      "--- [Meta-Retriever] Gathered 2 unique documents from 2 shards. ---\n",
      "--- [Generator] Synthesizing final answer... ---\n",
      "\n",
      "=============================================================\n",
      "                 SHARDED RAG SYSTEM OUTPUT\n",
      "=============================================================\n",
      "\n",
      "Retrieved Context:\n",
      "[Source: mkt-kb] Press Release: Unveiling the QuantumLeap V3, the AI processor that redefines speed. 'It's a game-changer for creative professionals,' says CEO Jane Doe. Available Q4.\n",
      "[Source: eng-kb] The QuantumLeap V3 processor utilizes a 3nm process node and features a dedicated AI accelerator core with 128 tensor units. API endpoint `/api/v3/status` provides real-time thermal throttling data.\n",
      "\n",
      "Final Answer:\n",
      "The QuantumLeap V3 is hailed as a 'game-changer for creative professionals' in its press release and is set to be available in Q4. On the technical side, it is built on a 3nm process and includes an API endpoint at `/api/v3/status` to monitor its real-time thermal status.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- [SHARDED RAG] Starting run... ---\")\n",
    "start_time = time.time()\n",
    "inputs = {\"question\": user_query}\n",
    "sharded_result = None\n",
    "for output in sharded_rag_app.stream(inputs, stream_mode=\"values\"):\n",
    "    sharded_result = output\n",
    "sharded_time = time.time() - start_time\n",
    "\n",
    "retrieved_context_sharded = format_docs(sharded_result['retrieved_docs'])\n",
    "sharded_answer = sharded_result['final_answer']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"                 SHARDED RAG SYSTEM OUTPUT\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "print(\"Retrieved Context:\")\n",
    "print(retrieved_context_sharded + \"\\n\")\n",
    "print(\"Final Answer:\")\n",
    "print(sharded_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
