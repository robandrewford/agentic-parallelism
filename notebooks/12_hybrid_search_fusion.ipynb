{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d1695c1",
   "metadata": {},
   "source": [
    "# Notebook 12 (Industrial Edition): Parallel Hybrid Search Fusion for RAG\n",
    "\n",
    "## Introduction: The Best of Both Worlds for High-Fidelity Retrieval\n",
    "\n",
    "This notebook explores an advanced RAG pattern designed to maximize retrieval **accuracy and precision**: **Parallel Hybrid Search Fusion**. This technique addresses the fact that no single search algorithm is perfect. Vector search excels at capturing semantic meaning, while traditional keyword search is unbeatable for finding exact matches. A hybrid system runs both in parallel and fuses their results, creating a more robust and comprehensive retrieval engine.\n",
    "\n",
    "### The Core Concept: Fusing Lexical and Semantic Search\n",
    "\n",
    "1.  **Vector Search (Semantic):** Finds documents that are contextually related to the query, even if they don't share any keywords.\n",
    "2.  **Keyword Search (Lexical):** Finds documents that contain the exact terms from the query (e.g., product codes, specific names, acronyms).\n",
    "\n",
    "By running both searches simultaneously and then combining their unique findings, we create a context that is both semantically relevant and factually precise. This fusion step is critical for preventing failures where one search method misses a crucial document that the other would have found.\n",
    "\n",
    "### Role in a Large-Scale System: Fusing Diverse Knowledge Types for High-Fidelity Context\n",
    "\n",
    "In any real-world knowledge base, information is a mix of prose, tables, and identifiers. A hybrid search system is essential for reliably accessing all of it:\n",
    "- **E-commerce Search:** Finding products by a descriptive query (\"warm winter coat\") and by a specific model number (\"XJ-2000\").\n",
    "- **Legal & Medical Research:** Retrieving documents based on a legal concept (semantic) and those that mention a specific case number or drug name (lexical).\n",
    "- **Technical Support:** Finding solutions based on a problem description and those that reference a specific error code.\n",
    "\n",
    "We will build and compare three RAG systems—Vector-Only, Keyword-Only, and Hybrid—to demonstrate how the hybrid approach retrieves a superior context and generates a more **complete and accurate** final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c85eea",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Environment\n",
    "\n",
    "We'll need our standard libraries plus `scikit-learn` for its TF-IDF implementation, which we'll use for our keyword search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229d673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain langgraph langsmith langchain-huggingface transformers accelerate bitsandbytes torch langchain-community sentence-transformers faiss-cpu scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd254de9",
   "metadata": {},
   "source": [
    "### 1.2: API Keys and Environment Configuration\n",
    "\n",
    "We will need our LangSmith and Hugging Face keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b71f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "_set_env(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n",
    "# Configure LangSmith for tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Industrial - RAG Hybrid Search\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d6de09",
   "metadata": {},
   "source": [
    "## Part 2: Components for the Hybrid Search System\n",
    "\n",
    "We'll need an LLM and two distinct retrieval mechanisms built on the same knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33712727",
   "metadata": {},
   "source": [
    "### 2.1: The Language Model (LLM)\n",
    "\n",
    "We will use `meta-llama/Meta-Llama-3-8B-Instruct` as our generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1925be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Initialized. Ready to power our RAG system.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", load_in_4bit=True)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=1024, do_sample=False)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(\"LLM Initialized. Ready to power our RAG system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c4249",
   "metadata": {},
   "source": [
    "### 2.2: Creating the Knowledge Base & Retrievers\n",
    "\n",
    "We will create a small knowledge base and then build two different retrievers on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe45a0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "kb_docs = [\n",
    "    Document(page_content=\"The QLeap-V4 processor is our latest flagship AI accelerator. It excels at large-scale parallel computations for deep learning workloads.\", metadata={\"source\": \"doc-1\"}),\n",
    "    Document(page_content=\"Project 'Titan' is a new initiative focused on developing energy-efficient hardware. The primary goal is to reduce the power consumption of our data centers.\", metadata={\"source\": \"doc-2\"}),\n",
    "    Document(page_content=\"A key feature of the QLeap-V4 is its advanced thermal management system. The official error code for overheating is 'ERR_THROTTLE_900'.\", metadata={\"source\": \"doc-3\"}),\n",
    "    Document(page_content=\"Our company is committed to sustainability. Project 'Titan' is a core part of our green computing strategy.\", metadata={\"source\": \"doc-4\"})\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b0ee8e",
   "metadata": {},
   "source": [
    "#### 2.2.1: The Vector Retriever (Semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2291b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store = FAISS.from_documents(kb_docs, embedding=embeddings)\n",
    "vector_retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b660f1cc",
   "metadata": {},
   "source": [
    "#### 2.2.2: The Keyword Retriever (Lexical)\n",
    "\n",
    "We will use `TfidfVectorizer` from `scikit-learn` to create a classic keyword search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87401146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from typing import List\n",
    "\n",
    "class TfidfRetriever(BaseRetriever):\n",
    "    vectorizer: TfidfVectorizer\n",
    "    docs: List[Document]\n",
    "    k: int = 2\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        doc_vectors = self.vectorizer.transform([doc.page_content for doc in self.docs])\n",
    "        similarities = cosine_similarity(query_vec, doc_vectors).flatten()\n",
    "        # Get top_k indices\n",
    "        top_k_indices = np.argsort(similarities)[-self.k:][::-1]\n",
    "        return [self.docs[i] for i in top_k_indices]\n",
    "\n",
    "vectorizer = TfidfVectorizer().fit([doc.page_content for doc in kb_docs])\n",
    "keyword_retriever = TfidfRetriever(vectorizer=vectorizer, docs=kb_docs, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5081c0",
   "metadata": {},
   "source": [
    "## Part 3: Building the RAG Systems\n",
    "\n",
    "We will build three RAG chains: two baselines and our advanced hybrid system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b72a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "generator_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert technical support agent. Answer the user's question based *only* on the provided context. If the context is insufficient, state that clearly.\\n\\nContext:\\n{context}\"),\n",
    "    (\"human\", \"Question: {question}\")\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(f\"[Source: {doc.metadata.get('source', 'N/A')}] {doc.page_content}\" for doc in docs)\n",
    "\n",
    "# --- RAG Chains ---\n",
    "rag_chain_vector = ({\"context\": vector_retriever | format_docs, \"question\": RunnablePassthrough()} | generator_prompt | llm | StrOutputParser())\n",
    "rag_chain_keyword = ({\"context\": keyword_retriever | format_docs, \"question\": RunnablePassthrough()} | generator_prompt | llm | StrOutputParser())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f88782",
   "metadata": {},
   "source": [
    "### 3.1: The Hybrid RAG System Graph\n",
    "\n",
    "This graph will run our two retrievers in parallel and then fuse their results before generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a708c411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid RAG graph compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List\n",
    "\n",
    "class HybridRAGState(TypedDict):\n",
    "    question: str\n",
    "    retrieved_docs: List[Document]\n",
    "    final_answer: str\n",
    "\n",
    "def parallel_retrieval_node(state: HybridRAGState):\n",
    "    \"\"\"Runs vector and keyword search in parallel.\"\"\"\n",
    "    print(\"--- [Hybrid Retriever] Running Vector and Keyword searches in parallel... ---\")\n",
    "    vector_docs = vector_retriever.invoke(state['question'])\n",
    "    keyword_docs = keyword_retriever.invoke(state['question'])\n",
    "    \n",
    "    # Fusion: Combine and deduplicate\n",
    "    all_docs = vector_docs + keyword_docs\n",
    "    unique_docs = list({doc.page_content: doc for doc in all_docs}.values())\n",
    "    print(f\"--- [Hybrid Retriever] Fused results: Found {len(unique_docs)} unique documents. ---\")\n",
    "    return {\"retrieved_docs\": unique_docs}\n",
    "\n",
    "def generation_node(state: HybridRAGState):\n",
    "    \"\"\"Generates the final answer.\"\"\"\n",
    "    print(\"--- [Generator] Synthesizing final answer from fused context... ---\")\n",
    "    context = format_docs(state['retrieved_docs'])\n",
    "    answer = (generator_prompt | llm | StrOutputParser()).invoke({\"context\": context, \"question\": state['question']})\n",
    "    return {\"final_answer\": answer}\n",
    "\n",
    "workflow = StateGraph(HybridRAGState)\n",
    "workflow.add_node(\"parallel_retrieval\", parallel_retrieval_node)\n",
    "workflow.add_node(\"generate_answer\", generation_node)\n",
    "\n",
    "workflow.set_entry_point(\"parallel_retrieval\")\n",
    "workflow.add_edge(\"parallel_retrieval\", \"generate_answer\")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "\n",
    "hybrid_rag_app = workflow.compile()\n",
    "print(\"Hybrid RAG graph compiled successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ab0a1",
   "metadata": {},
   "source": [
    "## Part 4: Head-to-Head Comparison\n",
    "\n",
    "Let's craft a query designed to fail for single-search systems. It will use a semantic concept (\"power saving efforts\") and a specific, exact keyword (`ERR_THROTTLE_900`). A good answer requires documents found by both search types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40516f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What are our company's power saving efforts, and what is the error code for QLeap-V4 overheating?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7862840",
   "metadata": {},
   "source": [
    "### 4.1: Running the Vector-Only RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e480a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================\n",
      "                VECTOR-ONLY RAG SYSTEM OUTPUT\n",
      "=============================================================\n",
      "\n",
      "Retrieved Documents:\n",
      "1. [Source: doc-2] Project 'Titan' is a new initiative focused on developing energy-efficient hardware. The primary goal is to reduce the power consumption of our data centers.\n",
      "2. [Source: doc-4] Our company is committed to sustainability. Project 'Titan' is a core part of our green computing strategy.\n",
      "\n",
      "Final Answer:\n",
      "Based on the provided context, the company's power saving effort is an initiative called Project 'Titan', which is focused on developing energy-efficient hardware to reduce power consumption in data centers and is part of the green computing strategy. The context does not contain information about an error code for QLeap-V4 overheating.\n"
     ]
    }
   ],
   "source": [
    "vector_docs = vector_retriever.invoke(user_query)\n",
    "vector_answer = rag_chain_vector.invoke(user_query)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"                VECTOR-ONLY RAG SYSTEM OUTPUT\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "print(\"Retrieved Documents:\")\n",
    "for i, doc in enumerate(vector_docs):\n",
    "    print(f\"{i+1}. {format_docs([doc])}\")\n",
    "\n",
    "print(\"Final Answer:\")\n",
    "print(vector_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a7f1b8",
   "metadata": {},
   "source": [
    "### 4.2: Running the Keyword-Only RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca5e24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================\n",
      "                KEYWORD-ONLY RAG SYSTEM OUTPUT\n",
      "=============================================================\n",
      "\n",
      "Retrieved Documents:\n",
      "1. [Source: doc-3] A key feature of the QLeap-V4 is its advanced thermal management system. The official error code for overheating is 'ERR_THROTTLE_900'.\n",
      "2. [Source: doc-2] Project 'Titan' is a new initiative focused on developing energy-efficient hardware. The primary goal is to reduce the power consumption of our data centers.\n",
      "\n",
      "Final Answer:\n",
      "Based on the context, the error code for QLeap-V4 overheating is 'ERR_THROTTLE_900'. Project 'Titan' is an initiative to reduce power consumption.\n"
     ]
    }
   ],
   "source": [
    "keyword_docs = keyword_retriever.invoke(user_query)\n",
    "keyword_answer = rag_chain_keyword.invoke(user_query)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"                KEYWORD-ONLY RAG SYSTEM OUTPUT\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "print(\"Retrieved Documents:\")\n",
    "for i, doc in enumerate(keyword_docs):\n",
    "    print(f\"{i+1}. {format_docs([doc])}\")\n",
    "\n",
    "print(\"Final Answer:\")\n",
    "print(keyword_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2c3f8e",
   "metadata": {},
   "source": [
    "### 4.3: Running the Hybrid Search RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbecce30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Hybrid Retriever] Running Vector and Keyword searches in parallel... ---\n",
      "--- [Hybrid Retriever] Fused results: Found 3 unique documents. ---\n",
      "--- [Generator] Synthesizing final answer from fused context... ---\n",
      "\n",
      "=============================================================\n",
      "                 HYBRID RAG SYSTEM OUTPUT\n",
      "=============================================================\n",
      "\n",
      "Retrieved Documents:\n",
      "1. [Source: doc-2] Project 'Titan' is a new initiative focused on developing energy-efficient hardware. The primary goal is to reduce the power consumption of our data centers.\n",
      "2. [Source: doc-4] Our company is committed to sustainability. Project 'Titan' is a core part of our green computing strategy.\n",
      "3. [Source: doc-3] A key feature of the QLeap-V4 is its advanced thermal management system. The official error code for overheating is 'ERR_THROTTLE_900'.\n",
      "\n",
      "Final Answer:\n",
      "Our company's power saving effort is called Project 'Titan', which is a core part of our green computing strategy aimed at developing energy-efficient hardware to reduce data center power consumption. The official error code for QLeap-V4 overheating is 'ERR_THROTTLE_900'.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": user_query}\n",
    "hybrid_result = None\n",
    "for output in hybrid_rag_app.stream(inputs, stream_mode=\"values\"):\n",
    "    hybrid_result = output\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"                 HYBRID RAG SYSTEM OUTPUT\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "print(\"Retrieved Documents:\")\n",
    "for i, doc in enumerate(hybrid_result['retrieved_docs']):\n",
    "    print(f\"{i+1}. {format_docs([doc])}\")\n",
    "\n",
    "print(\"Final Answer:\")\n",
    "print(hybrid_result['final_answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
