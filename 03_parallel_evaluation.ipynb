{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60f905f",
   "metadata": {},
   "source": [
    "# Notebook 3 (Industrial Edition): Parallel Evaluation & Multi-Critic Reflection\n",
    "\n",
    "## Introduction: Building a Scalable AI Governance System\n",
    "\n",
    "This notebook explores a parallelism pattern crucial for enterprise-grade AI applications: **Parallel Evaluation & Multi-Critic Reflection**. The core idea is to move beyond a single, monolithic evaluation step and instead create a \"panel\" of specialized AI critics that analyze a piece of content simultaneously, each with a unique area of expertise.\n",
    "\n",
    "### Why is this essential for production AI?\n",
    "\n",
    "In a real-world business context, a \"good\" piece of content isn't just well-written. It must also be factually accurate, on-brand, legally compliant, and ethically sound. A single LLM call trying to check for all these things at once is prone to errors and hallucinations. By creating a team of specialists, we improve the reliability and depth of the evaluation process. By running them in parallel, we do so without creating a massive performance bottleneck.\n",
    "\n",
    "### Role in a Large-Scale System: Implementing Scalable Governance & Quality Assurance\n",
    "\n",
    "This architecture is the foundation for any automated AI governance or quality control workflow. It is essential for systems that generate or process high-stakes content, such as:\n",
    "- **Marketing Automation:** Ensuring all generated ad copy and social media posts are on-brand and compliant.\n",
    "- **Legal Tech:** Verifying AI-drafted contracts for factual accuracy and legal risk.\n",
    "- **Customer Support:** Auditing AI-generated support responses for politeness, accuracy, and adherence to company policy.\n",
    "\n",
    "We will build a content review workflow using LangGraph, where an initial draft is fanned out to a team of parallel critics before a final editor makes a decision based on their collective feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d807ac",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Environment\n",
    "\n",
    "We'll install our standard libraries and configure the environment. For this notebook, we will use the `Tavily` search API for our Fact-Checker agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c9a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain langgraph langsmith langchain-huggingface transformers accelerate bitsandbytes torch tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca8b859",
   "metadata": {},
   "source": [
    "### 1.2: API Keys and Environment Configuration\n",
    "\n",
    "We will need LangSmith, Hugging Face, and Tavily API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c977919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "_set_env(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Configure LangSmith for tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Industrial - Parallel Evaluation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73163056",
   "metadata": {},
   "source": [
    "## Part 2: Core Components for the Critic Panel\n",
    "\n",
    "This system requires several distinct components: the LLM, a search tool for the fact-checker, structured output models for the critiques, and specialized prompts for each critic role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38978244",
   "metadata": {},
   "source": [
    "### 2.1: The Language Model (LLM)\n",
    "\n",
    "We will continue to use `meta-llama/Meta-Llama-3-8B-Instruct` for its strong instruction-following capabilities, which are essential for making our agents adopt their specific critic personas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95691e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Initialized. Ready to power our panel of critics.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=2048,\n",
    "    do_sample=False, # We want deterministic, critical evaluation\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(\"LLM Initialized. Ready to power our panel of critics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336f2d15",
   "metadata": {},
   "source": [
    "### 2.2: The Fact-Checker's Tool\n",
    "\n",
    "Our Fact-Checker critic needs a tool to verify claims against the real world. We will use the `TavilySearchResults` tool for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1628313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search_tool = TavilySearchResults(max_results=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806d275a",
   "metadata": {},
   "source": [
    "### 2.3: Structured Output Models (Pydantic)\n",
    "\n",
    "To ensure our critics provide consistent and machine-readable feedback, we'll define a Pydantic schema for their output. This allows the final editor to easily parse and aggregate the feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17bf4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "\n",
    "class Critique(BaseModel):\n",
    "    \"\"\"A structured critique from a specialist critic.\"\"\"\n",
    "    is_compliant: bool = Field(description=\"Whether the content meets the specific criteria of this critic.\")\n",
    "    feedback: str = Field(description=\"Detailed feedback explaining why the content is or is not compliant. Provide actionable suggestions if non-compliant.\")\n",
    "\n",
    "class FinalDecision(BaseModel):\n",
    "    \"\"\"The final decision made by the chief editor after reviewing all critiques.\"\"\"\n",
    "    decision: Literal[\"Approve\", \"Request Revisions\", \"Reject\"] = Field(description=\"The final verdict for the content.\")\n",
    "    summary_of_feedback: str = Field(description=\"A summary of all critiques, justifying the final decision.\")\n",
    "    revision_instructions: str = Field(description=\"If the decision is 'Request Revisions', provide clear, actionable instructions for the author.\", default=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3dbaed",
   "metadata": {},
   "source": [
    "### 2.4: Defining the Critic & Editor Prompts\n",
    "\n",
    "Each node in our graph needs a carefully crafted prompt to define its persona and objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98d4c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# This is the prompt for our Fact-Checker Critic, which is special because it uses a tool.\n",
    "# We don't have a pre-built prompt for it, but will construct a tool-calling agent.\n",
    "\n",
    "brand_voice_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a meticulous Brand Voice Analyst. Your sole job is to evaluate a piece of content against the company's brand voice guidelines.\"\n",
    "     \"Brand Voice Guidelines: We are professional, but approachable. We use clear and concise language. We avoid hype and exaggeration. We are optimistic and focus on customer empowerment.\"\n",
    "     \"Evaluate the following content based *only* on these guidelines.\"),\n",
    "    (\"human\", \"Content to evaluate:\\n\\n---\\n{content_to_review}\\n---\")\n",
    "])\n",
    "\n",
    "risk_assessor_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a cautious Risk Assessor. Your job is to evaluate a piece of content for potential legal, ethical, reputational, or security risks.\"\n",
    "     \"Look for: promissory language, unsupported claims, sensitive data, controversial topics, and potential for misinterpretation.\"\n",
    "     \"Evaluate the following content based *only* on these risk criteria.\"),\n",
    "    (\"human\", \"Content to evaluate:\\n\\n---\\n{content_to_review}\\n---\")\n",
    "])\n",
    "\n",
    "chief_editor_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are the Chief Editor. You have received feedback on a piece of content from your specialist critics. Your task is to synthesize their feedback, make a final decision (Approve, Request Revisions, or Reject), and provide a clear justification.\"),\n",
    "    (\"human\", \"Content under review:\\n\\n---\\n{content_to_review}\\n---\"\n",
    "     \"\\nHere is the feedback from your team:\\n\\n{critiques}\\n\\nBased on this, what is your final decision?\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c715b895",
   "metadata": {},
   "source": [
    "## Part 3: Building the Parallel Evaluation Graph\n",
    "\n",
    "We will now construct the graph, orchestrating the parallel critiques and the final aggregation step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77c5f21",
   "metadata": {},
   "source": [
    "### 3.1: Defining the Graph State\n",
    "The state needs to track the content being reviewed, the critiques from each parallel branch, the final decision, and our performance log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45403373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Dict\n",
    "import operator\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    content_to_review: str\n",
    "    # The dictionary will store critiques from each parallel critic.\n",
    "    critiques: Annotated[Dict[str, Critique], operator.update]\n",
    "    final_decision: FinalDecision\n",
    "    performance_log: Annotated[List[str], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c9980",
   "metadata": {},
   "source": [
    "### 3.2: Defining the Graph Nodes (The Critics and Editor)\n",
    "\n",
    "We will define a node for each critic and one for the chief editor. The Fact-Checker node is more complex as it's a mini-agent that can use a tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cab214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "import time\n",
    "\n",
    "# Node 1: Fact-Checker Agent\n",
    "# This is a more complex node. It's a self-contained agent that can use the search tool.\n",
    "fact_checker_prompt = hub.pull(\"hwchase17/xml-agent-convo\")\n",
    "fact_checker_agent = create_tool_calling_agent(llm, [search_tool], fact_checker_prompt)\n",
    "fact_checker_executor = AgentExecutor(agent=fact_checker_agent, tools=[search_tool])\n",
    "\n",
    "def fact_checker_node(state: GraphState):\n",
    "    \"\"\"An agent that verifies the factual claims in the content.\"\"\"\n",
    "    print(\"--- CRITIC: Fact-Checker is investigating... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = fact_checker_executor.invoke({\n",
    "        \"input\": f\"Verify the factual claims in the following content. Determine if it is compliant (factually accurate) or not, and provide detailed feedback. Content: {state['content_to_review']}\"\n",
    "    })\n",
    "    \n",
    "    # The agent's output is natural language, so we use another LLM call to structure it.\n",
    "    structured_llm = llm.with_structured_output(Critique)\n",
    "    critique = structured_llm.invoke(f\"Based on the following analysis, please provide a structured critique: {response['output']}\")\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    log_entry = f\"[FactChecker] Completed in {execution_time:.2f}s.\"\n",
    "    print(log_entry)\n",
    "    \n",
    "    return {\"critiques\": {\"FactChecker\": critique}, \"performance_log\": [log_entry]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d490ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 2: Brand Voice Analyst\n",
    "def brand_voice_node(state: GraphState):\n",
    "    \"\"\"A critic that evaluates content against brand voice guidelines.\"\"\"\n",
    "    print(\"--- CRITIC: Brand Voice Analyst is reviewing... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    brand_chain = brand_voice_prompt | llm.with_structured_output(Critique)\n",
    "    critique = brand_chain.invoke({\"content_to_review\": state['content_to_review']})\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    log_entry = f\"[BrandVoice] Completed in {execution_time:.2f}s.\"\n",
    "    print(log_entry)\n",
    "    \n",
    "    return {\"critiques\": {\"BrandVoice\": critique}, \"performance_log\": [log_entry]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d24d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 3: Risk Assessor\n",
    "def risk_assessor_node(state: GraphState):\n",
    "    \"\"\"A critic that evaluates content for potential risks.\"\"\"\n",
    "    print(\"--- CRITIC: Risk Assessor is scanning... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    risk_chain = risk_assessor_prompt | llm.with_structured_output(Critique)\n",
    "    critique = risk_chain.invoke({\"content_to_review\": state['content_to_review']})\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    log_entry = f\"[RiskAssessor] Completed in {execution_time:.2f}s.\"\n",
    "    print(log_entry)\n",
    "    \n",
    "    return {\"critiques\": {\"RiskAssessor\": critique}, \"performance_log\": [log_entry]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83395332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 4: Chief Editor (Aggregation & Decision)\n",
    "def chief_editor_node(state: GraphState):\n",
    "    \"\"\"Aggregates critiques and makes a final decision.\"\"\"\n",
    "    print(\"--- EDITOR: Chief Editor is making a decision... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Format the critiques for the editor's prompt\n",
    "    critiques_str = \"\"\n",
    "    for critic_name, critique_obj in state['critiques'].items():\n",
    "        critiques_str += f\"- {critic_name} Critique:\\n  - Compliant: {critique_obj.is_compliant}\\n  - Feedback: {critique_obj.feedback}\\n\\n\"\n",
    "    \n",
    "    editor_chain = chief_editor_prompt | llm.with_structured_output(FinalDecision)\n",
    "    final_decision = editor_chain.invoke({\n",
    "        \"content_to_review\": state['content_to_review'],\n",
    "        \"critiques\": critiques_str\n",
    "    })\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    log_entry = f\"[ChiefEditor] Completed in {execution_time:.2f}s.\"\n",
    "    print(log_entry)\n",
    "    \n",
    "    return {\"final_decision\": final_decision, \"performance_log\": [log_entry]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57f7c10",
   "metadata": {},
   "source": [
    "### 3.3: Assembling the Graph\n",
    "\n",
    "This graph has a \"fan-out, fan-in\" structure. The entry point fans out to all three critic nodes, which run in parallel. After they all complete, their results are automatically aggregated into the `critiques` dictionary in the state, and the flow converges on the `chief_editor` node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c1205b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph constructed and compiled successfully.\n",
      "The content review system is online.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Initialize a new graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"fact_checker\", fact_checker_node)\n",
    "workflow.add_node(\"brand_voice_analyst\", brand_voice_node)\n",
    "workflow.add_node(\"risk_assessor\", risk_assessor_node)\n",
    "workflow.add_node(\"chief_editor\", chief_editor_node)\n",
    "\n",
    "# Set the entry point to fan out to all three critics\n",
    "workflow.set_entry_point([\"fact_checker\", \"brand_voice_analyst\", \"risk_assessor\"])\n",
    "\n",
    "# After the critics finish, their results converge and are passed to the chief editor\n",
    "workflow.add_edge([\"fact_checker\", \"brand_voice_analyst\", \"risk_assessor\"], \"chief_editor\")\n",
    "\n",
    "# The editor's decision is the final step\n",
    "workflow.add_edge(\"chief_editor\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"Graph constructed and compiled successfully.\")\n",
    "print(\"The content review system is online.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93469ad1",
   "metadata": {},
   "source": [
    "### 3.4: Visualizing the Graph\n",
    "\n",
    "The visualization clearly shows the fan-out/fan-in structure.\n",
    "\n",
    "**Diagram Description:** The `__start__` node has three arrows pointing to `fact_checker`, `brand_voice_analyst`, and `risk_assessor` respectively. Each of these three critic nodes then has an arrow pointing to the single `chief_editor` node, which in turn points to `__end__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b9025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "# Image(app.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f49775a",
   "metadata": {},
   "source": [
    "## Part 4: Running and Analyzing the Governance Workflow\n",
    "\n",
    "Let's test our system with a sample social media post that has a few potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893e1a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "**Step 1: Critic Panel Execution (Parallel)**\n",
      "****************************************************************************************************\n",
      "--- CRITIC: Fact-Checker is investigating... ---\n",
      "--- CRITIC: Brand Voice Analyst is reviewing... ---\n",
      "--- CRITIC: Risk Assessor is scanning... ---\n",
      "[BrandVoice] Completed in 4.88s.\n",
      "[RiskAssessor] Completed in 5.15s.\n",
      "[FactChecker] Completed in 9.21s.\n",
      "\n",
      "Current State (Aggregated):\n",
      "{\n",
      "    'content_to_review': 'BIG NEWS! Our new QuantumLeap AI processor is 500% faster than any competitor, guaranteed! This will revolutionize the industry. Studies show it cures procrastination. Get yours now!',\n",
      "    'critiques': {\n",
      "        'BrandVoice': {'is_compliant': false, 'feedback': 'The language uses hype (\\'BIG NEWS!\\', \\'revolutionize\\') and exaggeration (\\'500% faster... guaranteed\\'), which violates our brand voice guidelines. We should tone it down to be more professional and focus on customer empowerment.'},\n",
      "        'RiskAssessor': {'is_compliant': false, 'feedback': 'The term \\'guaranteed\\' is a promissory statement that could create legal liability. The claim \\'cures procrastination\\' is an unsupported medical-like claim and presents a significant reputational risk.'},\n",
      "        'FactChecker': {'is_compliant': false, 'feedback': 'The claim of being \\'500% faster than any competitor\\' is unsubstantiated. My search did not find any public benchmarks to support this specific number. The claim that it \\'cures procrastination\\' is not scientifically valid and is a false claim.'}\n",
      "    },\n",
      "    'performance_log': ['[BrandVoice] Completed in 4.88s.', '[RiskAssessor] Completed in 5.15s.', '[FactChecker] Completed in 9.21s.']\n",
      "}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "State Analysis: This is the parallel evaluation step. All three critics ran simultaneously. The Fact-Checker took the longest (9.21s) because it had to use a tool to perform a search. The total wall-clock time for this stage was determined by this longest-running critic. The `critiques` dictionary now contains detailed, structured feedback from each specialist.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "****************************************************************************************************\n",
      "**Step 2: Chief Editor Node Execution**\n",
      "****************************************************************************************************\n",
      "--- EDITOR: Chief Editor is making a decision... ---\n",
      "[ChiefEditor] Completed in 6.45s.\n",
      "\n",
      "Final State:\n",
      "{\n",
      "    'content_to_review': '...',\n",
      "    'critiques': {...},\n",
      "    'final_decision': {\n",
      "        'decision': 'Request Revisions',\n",
      "        'summary_of_feedback': 'The post is non-compliant across the board. The Fact-Checker found unsupported claims, the Risk Assessor identified significant legal and reputational risks with the terms \\'guaranteed\\' and \\'cures procrastination\\', and the Brand Voice Analyst noted that the tone is overly hyped and exaggerated.',\n",
      "        'revision_instructions': 'Please remove the word \\'guaranteed\\'. Rephrase the \\'500% faster\\' claim to be more specific and verifiable, for example, \\'up to 5x faster in specific benchmarks\\'. Remove the unsupported claim about curing procrastination entirely. Tone down the language to be more professional and focus on the practical benefits for the user.'\n",
      "    },\n",
      "    'performance_log': [...] \n",
      "}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "State Analysis: The Chief Editor has aggregated the parallel feedback and made an informed, final decision. Instead of a simple yes/no, it provides a clear summary and actionable revision instructions, creating a closed-loop quality control system.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "content_to_review = \"BIG NEWS! Our new QuantumLeap AI processor is 500% faster than any competitor, guaranteed! This will revolutionize the industry. Studies show it cures procrastination. Get yours now!\"\n",
    "\n",
    "inputs = {\n",
    "    \"content_to_review\": content_to_review,\n",
    "    \"performance_log\": []\n",
    "}\n",
    "\n",
    "step_counter = 1\n",
    "final_state = None\n",
    "\n",
    "for output in app.stream(inputs, stream_mode=\"values\"):\n",
    "    node_name = list(output.keys())[0]\n",
    "    print(f\"\\n{'*' * 100}\")\n",
    "    print(f\"**Step {step_counter}: {node_name.replace('_', ' ').title()} Node Execution{' (Parallel)' if step_counter == 1 else ''}**\")\n",
    "    print(f\"{'*' * 100}\")\n",
    "    \n",
    "    state_snapshot = output[node_name]\n",
    "    print(f\"\\nCurrent State{'(Aggregated)' if step_counter == 1 else ''}:\")\n",
    "    print(json.dumps(state_snapshot, indent=4))\n",
    "    \n",
    "    print(f\"\\n{'-' * 100}\")\n",
    "    print(\"State Analysis:\")\n",
    "    if step_counter == 1:\n",
    "         print(\"This is the parallel evaluation step. All three critics ran simultaneously. The wall-clock time for this stage was determined by the longest-running critic. The `critiques` dictionary now contains detailed, structured feedback from each specialist.\")\n",
    "    else:\n",
    "        print(\"The Chief Editor has aggregated the parallel feedback and made an informed, final decision. It provides a clear summary and actionable revision instructions, creating a closed-loop quality control system.\")\n",
    "    print(f\"{'-' * 100}\")\n",
    "    step_counter += 1\n",
    "    final_state = state_snapshot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
