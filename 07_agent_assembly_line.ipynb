{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5805e1",
   "metadata": {},
   "source": [
    "# Notebook 7 (Industrial Edition): Agent Assembly Line (High-Throughput Pipelining)\n",
    "\n",
    "## Introduction: Maximizing Throughput for High-Volume Tasks\n",
    "\n",
    "This notebook explores the **Agent Assembly Line**, a powerful parallelism pattern designed not to speed up a single task, but to massively increase the *throughput* of a continuous stream of tasks. This is the architecture of an AI-powered factory, where items move through a series of specialized stations, with all stations working in parallel on different items.\n",
    "\n",
    "### The Core Concept: Pipelined Parallelism\n",
    "\n",
    "Instead of one monolithic agent processing items one by one from start to finish, we break the process into a sequence of specialized agents. As soon as `Agent A` finishes its job on `Item 1`, it passes the item to `Agent B` and immediately starts working on `Item 2`. `Agent B` works on `Item 1` while `Agent A` works on `Item 2`. This keeps all agents in the pipeline busy, maximizing the number of items processed per unit of time.\n",
    "\n",
    "### Role in a Large-Scale System: Enabling High-Throughput, Continuous Data Processing\n",
    "\n",
    "This pattern is fundamental for any system that needs to process a large volume of data in a structured, multi-step way. It's not about the fastest response for one user; it's about handling millions of events per day.\n",
    "- **Content Moderation:** A pipeline of agents for text filtering, image analysis, and video scanning.\n",
    "- **Data Enrichment:** Processing a stream of customer signups through stages like validation, data lookup, and lead scoring.\n",
    "- **Log Analysis:** A pipeline that ingests, categorizes, summarizes, and flags anomalies in server logs.\n",
    "\n",
    "We will build a three-stage pipeline to process a batch of product reviews. We will carefully analyze the timing to demonstrate how pipelining dramatically increases throughput compared to a traditional, sequential approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47950dcc",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Environment\n",
    "\n",
    "We'll install our standard libraries. This notebook is self-contained and does not require external tool APIs, as the focus is on the workflow orchestration itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968bd9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain langgraph langsmith langchain-huggingface transformers accelerate bitsandbytes torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b0077",
   "metadata": {},
   "source": [
    "### 1.2: API Keys and Environment Configuration\n",
    "\n",
    "We will need our LangSmith and Hugging Face keys for tracing and model access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eb3535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "_set_env(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n",
    "# Configure LangSmith for tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Industrial - Agent Assembly Line\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d3e20e",
   "metadata": {},
   "source": [
    "## Part 2: Components of the Assembly Line\n",
    "\n",
    "Our pipeline will process product reviews. We need to define the data structures that will move along the line and the specialist agents for each station."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ff3a10",
   "metadata": {},
   "source": [
    "### 2.1: The Language Model (LLM)\n",
    "\n",
    "We will use `meta-llama/Meta-Llama-3-8B-Instruct` as the engine for all agents in our pipeline. We will use a smaller `max_new_tokens` as each agent's task is focused and produces a short output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16aad4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Initialized. Ready to power our assembly line.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512, do_sample=False)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(\"LLM Initialized. Ready to power our assembly line.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c070755b",
   "metadata": {},
   "source": [
    "### 2.2: Structured Data Models (Pydantic)\n",
    "\n",
    "We need schemas to represent the data as it's transformed at each stage of the pipeline. This ensures a consistent data contract between our agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bcc55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "class TriageResult(BaseModel):\n",
    "    \"\"\"The result of the initial triage step.\"\"\"\n",
    "    category: Literal[\"Feedback\", \"Bug Report\", \"Support Request\", \"Irrelevant\"] = Field(description=\"The category of the review.\")\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    \"\"\"A concise summary of a product review.\"\"\"\n",
    "    summary: str = Field(description=\"A one-sentence summary of the key feedback in the review.\")\n",
    "\n",
    "class ExtractedData(BaseModel):\n",
    "    \"\"\"Structured data extracted from a product review summary.\"\"\"\n",
    "    product_mentioned: str = Field(description=\"The specific product the review is about.\")\n",
    "    sentiment: Literal[\"Positive\", \"Negative\", \"Neutral\"] = Field(description=\"The overall sentiment of the review.\")\n",
    "    key_feature: str = Field(description=\"The main feature or aspect discussed in the review.\")\n",
    "\n",
    "class ProcessedReview(BaseModel):\n",
    "    \"\"\"The final, fully processed review object.\"\"\"\n",
    "    original_review: str\n",
    "    category: str\n",
    "    summary: Optional[str] = None\n",
    "    extracted_data: Optional[ExtractedData] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142fc3ea",
   "metadata": {},
   "source": [
    "### 2.3: Defining the Pipeline Agent Prompts\n",
    "\n",
    "Each agent in our assembly line gets a highly specialized prompt for its specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c80ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "triage_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a Triage Specialist. Your job is to read a user review and categorize it into one of four categories: Feedback, Bug Report, Support Request, or Irrelevant.\"),\n",
    "    (\"human\", \"Please categorize the following review:\\n\\n---\\n{review_text}\\n---\")\n",
    "])\n",
    "\n",
    "summarizer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a Summarization Specialist. Your job is to read a user review and write a clear, one-sentence summary of its main point.\"),\n",
    "    (\"human\", \"Please summarize the following review:\\n\\n---\\n{review_text}\\n---\")\n",
    "])\n",
    "\n",
    "extractor_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a Data Extraction Specialist. Your job is to read a review summary and extract the product mentioned, the sentiment, and the key feature discussed.\"),\n",
    "    (\"human\", \"Please extract structured data from the following summary:\\n\\n---\\n{summary_text}\\n---\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c28de57",
   "metadata": {},
   "source": [
    "### 2.4: Creating the Agent Chains\n",
    "\n",
    "We'll package our prompts and the structured-output LLM into reusable LangChain Expression Language (LCEL) chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe2bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "triage_chain = triage_prompt | llm.with_structured_output(TriageResult)\n",
    "summarizer_chain = summarizer_prompt | llm.with_structured_output(Summary)\n",
    "extractor_chain = extractor_prompt | llm.with_structured_output(ExtractedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad890e96",
   "metadata": {},
   "source": [
    "## Part 3: Building the Pipelined Graph\n",
    "\n",
    "We will now construct the assembly line using LangGraph. The key is how we define the state and the nodes to operate on a *batch* of reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f236865",
   "metadata": {},
   "source": [
    "### 3.1: Defining the Graph State\n",
    "The state will hold the initial batch of reviews and the final list of fully processed reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66de43af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "\n",
    "class PipelineState(TypedDict):\n",
    "    # The initial batch of reviews to process\n",
    "    initial_reviews: List[str]\n",
    "    # The final list of processed reviews\n",
    "    processed_reviews: List[ProcessedReview]\n",
    "    # Performance log\n",
    "    performance_log: Annotated[List[str], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb834a49",
   "metadata": {},
   "source": [
    "### 3.2: Defining the Graph Nodes (The Assembly Stations)\n",
    "\n",
    "Each node represents a station on our assembly line. We will use Python's `ThreadPoolExecutor` to achieve true parallelism within each node, allowing each agent to process multiple items concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b44424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "MAX_WORKERS = 4 # Controls the degree of parallelism\n",
    "\n",
    "# Station 1: Triage\n",
    "def triage_node(state: PipelineState):\n",
    "    \"\"\"Categorizes all initial reviews in parallel.\"\"\"\n",
    "    print(f\"--- [Station 1: Triage] Processing {len(state['initial_reviews'])} reviews... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    triaged_reviews = []\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future_to_review = {executor.submit(triage_chain.invoke, {\"review_text\": review}): review for review in state['initial_reviews']}\n",
    "        for future in tqdm(as_completed(future_to_review), total=len(state['initial_reviews']), desc=\"Triage Progress\"):\n",
    "            original_review = future_to_review[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                triaged_reviews.append(ProcessedReview(original_review=original_review, category=result.category))\n",
    "            except Exception as exc:\n",
    "                print(f'Review generated an exception: {exc}')\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    log = f\"[Triage] Processed {len(state['initial_reviews'])} reviews in {execution_time:.2f}s.\"\n",
    "    print(log)\n",
    "    \n",
    "    return {\"processed_reviews\": triaged_reviews, \"performance_log\": [log]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772eeda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Station 2: Summarize (only operates on 'Feedback' reviews)\n",
    "def summarize_node(state: PipelineState):\n",
    "    \"\"\"Summarizes all reviews categorized as 'Feedback' in parallel.\"\"\"\n",
    "    feedback_reviews = [r for r in state['processed_reviews'] if r.category == \"Feedback\"]\n",
    "    if not feedback_reviews:\n",
    "        print(\"--- [Station 2: Summarizer] No feedback reviews to process. Skipping. ---\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"--- [Station 2: Summarizer] Processing {len(feedback_reviews)} feedback reviews... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    review_map = {r.original_review: r for r in state['processed_reviews']}\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future_to_review = {executor.submit(summarizer_chain.invoke, {\"review_text\": r.original_review}): r for r in feedback_reviews}\n",
    "        for future in tqdm(as_completed(future_to_review), total=len(feedback_reviews), desc=\"Summarizer Progress\"):\n",
    "            original_review_obj = future_to_review[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                # Update the object in our map\n",
    "                review_map[original_review_obj.original_review].summary = result.summary\n",
    "            except Exception as exc:\n",
    "                print(f'Review generated an exception: {exc}')\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    log = f\"[Summarizer] Processed {len(feedback_reviews)} reviews in {execution_time:.2f}s.\"\n",
    "    print(log)\n",
    "    \n",
    "    return {\"processed_reviews\": list(review_map.values()), \"performance_log\": [log]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95928387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Station 3: Extract Data (operates on summarized reviews)\n",
    "def extract_data_node(state: PipelineState):\n",
    "    \"\"\"Extracts structured data from all summarized reviews in parallel.\"\"\"\n",
    "    summarized_reviews = [r for r in state['processed_reviews'] if r.summary is not None]\n",
    "    if not summarized_reviews:\n",
    "        print(\"--- [Station 3: Extractor] No summarized reviews to process. Skipping. ---\")\n",
    "        return {}\n",
    "        \n",
    "    print(f\"--- [Station 3: Extractor] Processing {len(summarized_reviews)} summarized reviews... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    review_map = {r.original_review: r for r in state['processed_reviews']}\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future_to_review = {executor.submit(extractor_chain.invoke, {\"summary_text\": r.summary}): r for r in summarized_reviews}\n",
    "        for future in tqdm(as_completed(future_to_review), total=len(summarized_reviews), desc=\"Extractor Progress\"):\n",
    "            original_review_obj = future_to_review[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                review_map[original_review_obj.original_review].extracted_data = result\n",
    "            except Exception as exc:\n",
    "                print(f'Review generated an exception: {exc}')\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    log = f\"[Extractor] Processed {len(summarized_reviews)} reviews in {execution_time:.2f}s.\"\n",
    "    print(log)\n",
    "    \n",
    "    return {\"processed_reviews\": list(review_map.values()), \"performance_log\": [log]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8876e7",
   "metadata": {},
   "source": [
    "### 3.3: Assembling the Graph\n",
    "\n",
    "We connect the nodes in a simple sequence, forming our three-stage assembly line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb108ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph constructed and compiled successfully.\n",
      "The review processing assembly line is ready to run.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(PipelineState)\n",
    "\n",
    "workflow.add_node(\"triage\", triage_node)\n",
    "workflow.add_node(\"summarize\", summarize_node)\n",
    "workflow.add_node(\"extract_data\", extract_data_node)\n",
    "\n",
    "workflow.set_entry_point(\"triage\")\n",
    "workflow.add_edge(\"triage\", \"summarize\")\n",
    "workflow.add_edge(\"summarize\", \"extract_data\")\n",
    "workflow.add_edge(\"extract_data\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"Graph constructed and compiled successfully.\")\n",
    "print(\"The review processing assembly line is ready to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5793f3",
   "metadata": {},
   "source": [
    "### 3.4: Visualizing the Graph\n",
    "\n",
    "**Diagram Description:** The diagram shows a simple, linear graph: `__start__` -> `triage` -> `summarize` -> `extract_data` -> `__end__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728c09ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "# Image(app.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752f9ab7",
   "metadata": {},
   "source": [
    "## Part 4: Running the Pipeline and Analyzing Throughput\n",
    "\n",
    "We'll create a batch of 10 sample reviews and run them through our assembly line. We'll pay close attention to the execution time of each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d55757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reviews = [\n",
    "    \"The Aura Smart Ring's battery life is incredible, easily lasts a week! Best sleep tracker I've ever used.\", # Feedback\n",
    "    \"My QuantumLeap processor arrived with a bent pin, and the box was crushed. I need a replacement ASAP.\", # Support Request\n",
    "    \"Love the new Smart Mug, but the app keeps crashing on my Android phone whenever I try to set a custom temperature.\", # Bug Report\n",
    "    \"The titanium finish on the Aura Ring feels so premium. It's lightweight and looks amazing.\", # Feedback\n",
    "    \"I think I was overcharged for my last order (A123). Can someone please check my invoice?\", # Support Request\n",
    "    \"The personalized energy suggestions from the Smart Mug are surprisingly accurate. A great feature!\", # Feedback\n",
    "    \"This is not a product review, I just wanted to say your website is very well designed.\", # Irrelevant\n",
    "    \"The QuantumLeap is fast, but it runs way too hot. The fan noise is a real problem under load.\", # Feedback\n",
    "    \"The app for the Aura Ring fails to sync my sleep data about half the time. I have to restart my phone to fix it.\", # Bug Report\n",
    "    \"I wish the Smart Mug came in more colors. A matte black option would be perfect.\", # Feedback\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37aef8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Station 1: Triage] Processing 10 reviews... ---\n",
      "Triage Progress: 100%|██████████| 10/10 [00:09<00:00,  1.10it/s]\n",
      "[Triage] Processed 10 reviews in 9.12s.\n",
      "--- [Station 2: Summarizer] Processing 5 feedback reviews... ---\n",
      "Summarizer Progress: 100%|██████████| 5/5 [00:05<00:00,  1.05it/s]\n",
      "[Summarizer] Processed 5 reviews in 5.23s.\n",
      "--- [Station 3: Extractor] Processing 5 summarized reviews... ---\n",
      "Extractor Progress: 100%|██████████| 5/5 [00:06<00:00,  1.21it/s]\n",
      "[Extractor] Processed 5 reviews in 6.05s.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"initial_reviews\": sample_reviews,\n",
    "    \"processed_reviews\": []\n",
    "}\n",
    "\n",
    "final_state = None\n",
    "for output in app.stream(inputs, stream_mode=\"values\"):\n",
    "    final_state = output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c49a1ea",
   "metadata": {},
   "source": [
    "## Part 5: Final Analysis - The Power of Pipelining\n",
    "\n",
    "Let's look at the final processed data and, most importantly, analyze the performance to understand the concept of throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c7f45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================\n",
      "                FINAL PROCESSED DATA (Sample)\n",
      "=============================================================\n",
      "\n",
      "{\n",
      "    \"original_review\": \"The Aura Smart Ring's battery life is incredible, easily lasts a week! Best sleep tracker I've ever used.\",\n",
      "    \"category\": \"Feedback\",\n",
      "    \"summary\": \"The Aura Smart Ring has excellent week-long battery life and is a top-tier sleep tracker.\",\n",
      "    \"extracted_data\": {\n",
      "        \"product_mentioned\": \"Aura Smart Ring\",\n",
      "        \"sentiment\": \"Positive\",\n",
      "        \"key_feature\": \"Battery Life\"\n",
      "    }\n",
      "}\n",
      "-------------------------------------------------------------\n",
      "{\n",
      "    \"original_review\": \"My QuantumLeap processor arrived with a bent pin, and the box was crushed. I need a replacement ASAP.\",\n",
      "    \"category\": \"Support Request\",\n",
      "    \"summary\": null,\n",
      "    \"extracted_data\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(\"=\"*60)\n",
    "print(\"                FINAL PROCESSED DATA (Sample)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in [0, 1]: # Print a sample of two processed reviews\n",
    "    print(json.dumps(final_state['processed_reviews'][i], indent=4, default=lambda o: o.dict() if hasattr(o, 'dict') else o))\n",
    "    print(\"-\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
