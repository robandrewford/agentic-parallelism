{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a568e668",
   "metadata": {},
   "source": [
    "# Notebook 10 (Industrial Edition): Parallel Query Expansion for RAG\n",
    "\n",
    "## Introduction: Overcoming the Vocabulary Mismatch Problem\n",
    "\n",
    "This notebook dives into our first advanced Retrieval-Augmented Generation (RAG) pattern: **Parallel Query Expansion & Transformation**. This pre-retrieval strategy is designed to solve one of the most common failure modes in RAG systems: the **vocabulary mismatch problem**. Users often don't know the precise terminology used in the knowledge base, causing simple searches to miss relevant documents.\n",
    "\n",
    "### The Core Concept: Searching with Multiple Perspectives\n",
    "\n",
    "Instead of taking the user's query at face value, we use a powerful LLM to brainstorm multiple, diverse ways to search for the same information. In a single, parallel-planned step, we can generate:\n",
    "1.  **Hypothetical Documents (HyDE):** A generated answer that is likely to be semantically similar to the true answer documents.\n",
    "2.  **Sub-Questions:** Decomposing a complex query into smaller, more specific questions.\n",
    "3.  **Keyword & Entity Extraction:** Identifying the core concepts for a precise search.\n",
    "\n",
    "By searching with all these perspectives at once, we dramatically increase the **recall** of our retrieval stepâ€”finding more of the relevant documents available.\n",
    "\n",
    "### Role in a Large-Scale System: Maximizing Knowledge Discovery & Improving Accuracy\n",
    "\n",
    "In any large-scale RAG system (e.g., an enterprise search engine, a research assistant), this pattern is critical for ensuring that the system is robust to user phrasing and can uncover all relevant information. Better recall at the retrieval stage directly leads to a more informed, comprehensive, and **accurate** final answer from the generator.\n",
    "\n",
    "We will build and compare two RAG systems: a simple one using the raw query, and an advanced one using parallel query expansion. We will demonstrate that the advanced system produces a measurably more accurate and complete answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35f6bfa",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Environment\n",
    "\n",
    "We'll need our standard libraries plus `langchain-community` for vector stores and embeddings to build our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83e2987",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain langgraph langsmith langchain-huggingface transformers accelerate bitsandbytes torch langchain-community sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c47a83e",
   "metadata": {},
   "source": [
    "### 1.2: API Keys and Environment Configuration\n",
    "\n",
    "We will need our LangSmith and Hugging Face keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa8edeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "_set_env(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n",
    "# Configure LangSmith for tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Industrial - RAG Query Expansion\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a016078",
   "metadata": {},
   "source": [
    "## Part 2: Components for the Advanced RAG System\n",
    "\n",
    "We'll need an LLM, a knowledge base, and the structured output models that define our parallel query transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e322492",
   "metadata": {},
   "source": [
    "### 2.1: The Language Model (LLM)\n",
    "\n",
    "We will use `meta-llama/Meta-Llama-3-8B-Instruct` for its strong instruction-following and structured data generation capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2de87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Initialized. Ready to power our RAG system.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", load_in_4bit=True)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=2048, do_sample=False)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(\"LLM Initialized. Ready to power our RAG system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89d294f",
   "metadata": {},
   "source": [
    "### 2.2: Creating the Knowledge Base\n",
    "\n",
    "We'll create a small, targeted knowledge base about AI architectures. The documents will use specific terminology that a user might not know, creating the perfect scenario to test our query expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bc3a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Base created with 3 documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. Create knowledge base documents with specific terminology\n",
    "kb_docs = [\n",
    "    \"**Multi-headed Attention Mechanism**: The core component of the Transformer architecture is the multi-headed self-attention mechanism. It allows the model to weigh the importance of different words in the input sequence when processing a particular word, capturing contextual relationships. Each 'head' learns a different set of attention patterns in parallel.\",\n",
    "    \"**Mixture of Experts (MoE) Layers**: To scale up model size without a proportional increase in computational cost, some large language models employ Mixture of Experts layers. In an MoE layer, a router network dynamically selects a small subset of 'expert' sub-networks to process each input token. This allows for a very high parameter count while keeping inference costs manageable.\",\n",
    "    \"**FlashAttention Optimization**: A significant performance bottleneck in training large Transformers is the memory bandwidth required by the attention mechanism. FlashAttention is an I/O-aware algorithm that reorders the computation to reduce the number of read/write operations to high-bandwidth memory (HBM), leading to substantial speedups.\"\n",
    "]\n",
    "\n",
    "# 2. Create an embedding model and vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_texts(kb_docs, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "print(f\"Knowledge Base created with {len(kb_docs)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7dd148",
   "metadata": {},
   "source": [
    "### 2.3: Structured Data Models for Query Expansion\n",
    "\n",
    "This Pydantic model defines the structure of our parallel query generation. The LLM will be instructed to populate all fields of this model in a single call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84128b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class ExpandedQueries(BaseModel):\n",
    "    \"\"\"A set of diverse, expanded queries to improve retrieval recall.\"\"\"\n",
    "    hypothetical_document: str = Field(description=\"A generated, paragraph-length hypothetical document that directly answers the user's question, which will be used for semantic search.\", alias=\"hyde_query\")\n",
    "    sub_questions: List[str] = Field(description=\"A list of 2-3 smaller, more specific questions that break down the original query.\")\n",
    "    keywords: List[str] = Field(description=\"A list of 3-5 core keywords and entities extracted from the user's query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e797bf5c",
   "metadata": {},
   "source": [
    "## Part 3: Building the RAG Systems\n",
    "\n",
    "We will now build two complete RAG chains: a simple baseline and our advanced, expanded version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3942ba",
   "metadata": {},
   "source": [
    "### 3.1: The Simple RAG System (Baseline)\n",
    "\n",
    "This is a standard RAG implementation. It takes the user's query, retrieves documents, and then passes them to a generator to produce an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d7a70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "generator_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert AI Architect. Answer the user's question based *only* on the following context. If the context does not contain the answer, say so. Be concise and accurate.\\n\\nContext:\\n{context}\"),\n",
    "    (\"human\", \"Question: {question}\")\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "simple_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | generator_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92abc15e",
   "metadata": {},
   "source": [
    "### 3.2: The Advanced RAG System (with Parallel Query Expansion)\n",
    "\n",
    "This system uses a LangGraph graph to orchestrate the pre-retrieval query expansion step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017606e3",
   "metadata": {},
   "source": [
    "#### 3.2.1: Graph State and Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d82f146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Optional\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class RAGGraphState(TypedDict):\n",
    "    original_question: str\n",
    "    expanded_queries: Optional[ExpandedQueries]\n",
    "    retrieved_docs: List[Document]\n",
    "    final_answer: str\n",
    "\n",
    "# The Query Expansion Node\n",
    "query_expansion_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a query expansion specialist. Your goal is to transform a user's question into a diverse set of search queries to maximize retrieval recall. Generate a hypothetical document, sub-questions, and keywords.\"),\n",
    "    (\"human\", \"Please expand the following question: {question}\")\n",
    "])\n",
    "query_expansion_chain = query_expansion_prompt | llm.with_structured_output(ExpandedQueries)\n",
    "\n",
    "def query_expansion_node(state: RAGGraphState):\n",
    "    print(\"--- [Expander] Generating parallel queries... ---\")\n",
    "    expanded_queries = query_expansion_chain.invoke({\"question\": state['original_question']})\n",
    "    return {\"expanded_queries\": expanded_queries}\n",
    "\n",
    "# The Retrieval Node (with parallel execution)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def retrieval_node(state: RAGGraphState):\n",
    "    print(\"--- [Retriever] Executing parallel searches... ---\")\n",
    "    all_queries = []\n",
    "    expanded = state['expanded_queries']\n",
    "    all_queries.append(expanded.hypothetical_document)\n",
    "    all_queries.extend(expanded.sub_questions)\n",
    "    all_queries.extend(expanded.keywords)\n",
    "    \n",
    "    all_docs = []\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        # Run all retrievals in parallel\n",
    "        results = executor.map(retriever.invoke, all_queries)\n",
    "        for docs in results:\n",
    "            all_docs.extend(docs)\n",
    "    \n",
    "    # Deduplicate documents based on page content\n",
    "    unique_docs = {doc.page_content: doc for doc in all_docs}.values()\n",
    "    print(f\"--- [Retriever] Found {len(unique_docs)} unique documents from {len(all_queries)} queries. ---\")\n",
    "    return {\"retrieved_docs\": list(unique_docs)}\n",
    "\n",
    "# The Generation Node\n",
    "def generation_node(state: RAGGraphState):\n",
    "    print(\"--- [Generator] Synthesizing final answer... ---\")\n",
    "    context = format_docs(state['retrieved_docs'])\n",
    "    answer = (\n",
    "        generator_prompt \n",
    "        | llm \n",
    "        | StrOutputParser()\n",
    "    ).invoke({\"context\": context, \"question\": state['original_question']})\n",
    "    return {\"final_answer\": answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0886d9",
   "metadata": {},
   "source": [
    "#### 3.2.2: Assembling the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5403b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced RAG graph compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(RAGGraphState)\n",
    "\n",
    "workflow.add_node(\"expand_queries\", query_expansion_node)\n",
    "workflow.add_node(\"retrieve_docs\", retrieval_node)\n",
    "workflow.add_node(\"generate_answer\", generation_node)\n",
    "\n",
    "workflow.set_entry_point(\"expand_queries\")\n",
    "workflow.add_edge(\"expand_queries\", \"retrieve_docs\")\n",
    "workflow.add_edge(\"retrieve_docs\", \"generate_answer\")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "\n",
    "advanced_rag_app = workflow.compile()\n",
    "print(\"Advanced RAG graph compiled successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8cbe9a",
   "metadata": {},
   "source": [
    "## Part 4: Head-to-Head RAG Comparison\n",
    "\n",
    "Now we will ask both systems the same question. The question is intentionally phrased using general terms (\"make models bigger and faster\") that do not appear in our knowledge base, forcing the advanced system to leverage its expansion capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf226c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"How do modern AI systems get so big and fast at the same time? I've heard about attention but I'm not sure how it's optimized.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a4e90b",
   "metadata": {},
   "source": [
    "### 4.1: Running the Simple RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f23663e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [SIMPLE RAG] Retrieving documents...\n",
      "--- [SIMPLE RAG] Documents Retrieved: 1\n",
      "--- [SIMPLE RAG] Generating answer...\n",
      "\n",
      "=============================================================\n",
      "                  SIMPLE RAG SYSTEM OUTPUT\n",
      "=============================================================\n",
      "\n",
      "Based on the context, one core component of the Transformer architecture is the multi-headed self-attention mechanism. It helps the model understand contextual relationships by weighing the importance of different words in a sequence. Each 'head' learns different attention patterns in parallel.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- [SIMPLE RAG] Retrieving documents...\")\n",
    "# We intercept the retrieval step to inspect the documents\n",
    "simple_retrieved_docs = retriever.invoke(user_query)\n",
    "print(f\"--- [SIMPLE RAG] Documents Retrieved: {len(simple_retrieved_docs)}\")\n",
    "print(\"--- [SIMPLE RAG] Generating answer...\\n\")\n",
    "\n",
    "simple_rag_answer = simple_rag_chain.invoke(user_query)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"                  SIMPLE RAG SYSTEM OUTPUT\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "print(simple_rag_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6495b82d",
   "metadata": {},
   "source": [
    "### 4.2: Running the Advanced RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d4aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Expander] Generating parallel queries... ---\n",
      "--- [Retriever] Executing parallel searches... ---\n",
      "--- [Retriever] Found 3 unique documents from 9 queries. ---\n",
      "--- [Generator] Synthesizing final answer... ---\n",
      "\n",
      "=============================================================\n",
      "                 ADVANCED RAG SYSTEM OUTPUT\n",
      "=============================================================\n",
      "\n",
      "Modern AI systems get big and fast through a combination of architectural innovations and optimization algorithms. Two key techniques are:\n",
      "\n",
      "1.  **Mixture of Experts (MoE) Layers**: This architecture allows models to have a very high parameter count (making them 'big') without a proportional increase in computation. A router network dynamically selects a small group of 'expert' sub-networks to process each input, keeping inference costs low.\n",
      "\n",
      "2.  **FlashAttention**: This is an I/O-aware algorithm that specifically optimizes the attention mechanism to make it 'fast'. It reduces the number of memory read/write operations, which is a major performance bottleneck, leading to significant speedups in training and inference.\n",
      "\n",
      "The multi-headed attention mechanism is the core of the Transformer, allowing it to understand context.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"original_question\": user_query}\n",
    "advanced_rag_result = None\n",
    "for output in advanced_rag_app.stream(inputs, stream_mode=\"values\"):\n",
    "    advanced_rag_result = output\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"                 ADVANCED RAG SYSTEM OUTPUT\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "print(advanced_rag_result['final_answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
