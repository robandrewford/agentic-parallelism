{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4636fab0",
   "metadata": {},
   "source": [
    "# Notebook 13 (Industrial Edition): Parallel Context Pre-processing for RAG\n",
    "\n",
    "## Introduction: Optimizing the Context for Performance and Accuracy\n",
    "\n",
    "This notebook explores a critical post-retrieval RAG pattern: **Parallel Context Pre-processing**. A common RAG strategy is to retrieve a large number of documents (high recall) to ensure the answer is present. However, stuffing all these documents into a single, massive context for the final generator is inefficient and can harm quality. It's slow, expensive, and susceptible to the \"lost in the middle\" problem, where LLMs struggle to find relevant facts buried in a sea of text.\n",
    "\n",
    "### The Core Concept: Distill Before You Synthesize\n",
    "\n",
    "This pattern introduces an intermediate \"distillation\" step between retrieval and generation. After retrieving a large set of candidate documents, we use multiple, small, parallel LLM calls to process them. Each call acts as a highly-focused filter or summarizer for a subset of the documents. The goal is to distill the raw, noisy context into a smaller, denser, and more relevant context. Only this high-quality context is then passed to the final, expensive generator LLM.\n",
    "\n",
    "### Role in a Large-Scale System: Optimizing the LLM's Contextual Input for Performance & Cost\n",
    "\n",
    "This is a crucial optimization layer for any production RAG system that prioritizes cost, latency, and accuracy:\n",
    "- **Cost Reduction:** The final generation step, often using the most powerful and expensive model, operates on a much smaller context, significantly reducing token costs.\n",
    "- **Latency Improvement:** The final LLM call is much faster with a smaller prompt. The parallel distillation step's latency is often less than the savings it creates.\n",
    "- **Accuracy Enhancement:** By filtering out irrelevant or distracting documents, we provide a clean, focused context to the generator. This reduces the chance of hallucination and helps the model produce a more precise and correct answer.\n",
    "\n",
    "We will build and compare two RAG systems: one that uses a large, raw context and another that uses a parallel pre-processing step. We will demonstrate the improvements in **latency, cost (token usage), and final answer accuracy**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d259a06b",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ecea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain langgraph langsmith langchain-huggingface transformers accelerate bitsandbytes torch langchain-community sentence-transformers faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f52b01",
   "metadata": {},
   "source": [
    "### 1.2: API Keys and Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a4bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "_set_env(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Industrial - RAG Context Pre-processing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450f6738",
   "metadata": {},
   "source": [
    "## Part 2: Components for the Context-Aware RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cf2263",
   "metadata": {},
   "source": [
    "### 2.1: The Language Model (LLM)\n",
    "\n",
    "We will use `meta-llama/Meta-Llama-3-8B-Instruct` for all our generation and distillation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90b7cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Initialized. Ready to power our RAG system.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", load_in_4bit=True)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=2048, do_sample=False)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(\"LLM Initialized. Ready to power our RAG system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e7c52",
   "metadata": {},
   "source": [
    "### 2.2: Creating the Knowledge Base\n",
    "\n",
    "We'll create a slightly larger knowledge base with some documents that are only tangentially related to each other. This will create a scenario where a high-recall retrieval step pulls in some noisy, irrelevant documents, making the distillation step necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d13b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Base created with 10 documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "kb_docs = [\n",
    "    Document(page_content=\"The QLeap-V4 processor, released in 2023, is our flagship AI accelerator. Its primary use case is for training large language models.\", metadata={\"source\": \"QL-V4-SpecSheet\"}),\n",
    "    Document(page_content=\"A key feature of the QLeap-V4 is its advanced thermal management system. The official error code for overheating is 'ERR_THROTTLE_900'.\", metadata={\"source\": \"QL-V4-Troubleshooting\"}),\n",
    "    Document(page_content=\"For optimal performance with the QLeap-V4, a power supply unit of at least 1200W is recommended.\", metadata={\"source\": \"QL-V4-HardwareGuide\"}),\n",
    "    Document(page_content=\"Our previous generation chip, the QLeap-V3 (released in 2021), had a known issue with its memory controller that was fixed in later revisions.\", metadata={\"source\": \"QL-V3-KnownIssues\"}),\n",
    "    Document(page_content=\"The Aura Smart Ring uses a photoplethysmography (PPG) sensor to measure heart rate.\", metadata={\"source\": \"Aura-TechSpec\"}),\n",
    "    Document(page_content=\"The official price for the QLeap-V4 is $1,999 USD. Educational and volume discounts are available.\", metadata={\"source\": \"QL-V4-Pricing\"}),\n",
    "    Document(page_content=\"Software drivers for the QLeap-V4 are available for Linux and Windows. The latest driver version is 512.77.\", metadata={\"source\": \"QL-V4-Downloads\"}),\n",
    "    Document(page_content=\"Project 'Titan' is our company's initiative to develop energy-efficient hardware, but it is a separate research project from the QLeap product line.\", metadata={\"source\": \"Project-Titan-FAQ\"}),\n",
    "    Document(page_content=\"Warranty claims for the QLeap-V4 processor must be filed within 2 years of the purchase date.\", metadata={\"source\": \"QL-V4-Warranty\"}),\n",
    "    Document(page_content=\"The QLeap-V3 chip had a recommended power supply of 800W.\", metadata={\"source\": \"QL-V3-HardwareGuide\"})\n",
    "]\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(kb_docs, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10}) # High recall retriever\n",
    "\n",
    "print(f\"Knowledge Base created with {len(kb_docs)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f95d5",
   "metadata": {},
   "source": [
    "### 2.3: Components for Token Counting\n",
    "\n",
    "To measure cost savings, we need a way to count the number of tokens in a prompt. We'll use the `tiktoken` library for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e6597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Counts the number of tokens in a string using tiktoken.\"\"\"\n",
    "    # Using a common encoding for estimation\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500819ea",
   "metadata": {},
   "source": [
    "## Part 3: Building the RAG Systems\n",
    "\n",
    "We'll build two systems: the simple, large-context baseline, and the advanced graph with the parallel distillation step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c9e17",
   "metadata": {},
   "source": [
    "### 3.1: The Simple RAG System (Baseline)\n",
    "\n",
    "This is a standard RAG chain that retrieves 10 documents and sends them all to the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ff5ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "generator_prompt_template = (\n",
    "    \"You are an expert technical support agent. Answer the user's question with high accuracy, based *only* on the following context. \"\n",
    "    \"If the context does not contain the answer, state that clearly.\\n\\n\"\n",
    "    \"Context:\\n{context}\\n\\nQuestion: {question}\"\n",
    ")\n",
    "generator_prompt = ChatPromptTemplate.from_template(generator_prompt_template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(f\"[Source: {doc.metadata.get('source', 'N/A')}] {doc.page_content}\" for doc in docs)\n",
    "\n",
    "simple_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | generator_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189f6ce2",
   "metadata": {},
   "source": [
    "### 3.2: The Advanced RAG System with Parallel Distillation\n",
    "\n",
    "This system uses a LangGraph graph to add a `distill_context` node between retrieval and generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf4c8fa",
   "metadata": {},
   "source": [
    "#### 3.2.1: Graph State and Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5095ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "class RAGGraphState(TypedDict):\n",
    "    question: str\n",
    "    raw_docs: List[Document]\n",
    "    distilled_docs: List[Document]\n",
    "    final_answer: str\n",
    "\n",
    "class RelevancyCheck(BaseModel):\n",
    "    \"\"\"A check for whether a document is relevant to a question.\"\"\"\n",
    "    is_relevant: bool = Field(description=\"True if the document contains information that directly helps answer the question.\")\n",
    "    brief_explanation: str = Field(description=\"A one-sentence explanation of why the document is or is not relevant.\")\n",
    "\n",
    "# Node 1: Retrieval\n",
    "def retrieval_node(state: RAGGraphState):\n",
    "    print(\"--- [Retriever] Retrieving initial set of 10 documents... ---\")\n",
    "    raw_docs = retriever.invoke(state['question'])\n",
    "    return {\"raw_docs\": raw_docs}\n",
    "\n",
    "# Node 2: Parallel Context Distillation\n",
    "distiller_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Given the user's question, determine if the following document is relevant for answering it. \"\n",
    "    \"Provide a brief explanation.\\n\\n\"\n",
    "    \"Question: {question}\\n\\nDocument:\\n{document}\"\n",
    ")\n",
    "distiller_chain = distiller_prompt | llm.with_structured_output(RelevancyCheck)\n",
    "\n",
    "def distill_context_node(state: RAGGraphState):\n",
    "    \"\"\"Scans all retrieved documents in parallel to filter for relevance.\"\"\"\n",
    "    print(f\"--- [Distiller] Pre-processing {len(state['raw_docs'])} raw documents in parallel... ---\")\n",
    "    \n",
    "    relevant_docs = []\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_to_doc = {executor.submit(distiller_chain.invoke, {\"question\": state['question'], \"document\": doc.page_content}): doc for doc in state['raw_docs']}\n",
    "        for future in as_completed(future_to_doc):\n",
    "            doc = future_to_doc[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result.is_relevant:\n",
    "                    print(f\"  - Doc '{doc.metadata['source']}' IS relevant. Reason: {result.brief_explanation}\")\n",
    "                    relevant_docs.append(doc)\n",
    "                else:\n",
    "                    print(f\"  - Doc '{doc.metadata['source']}' is NOT relevant. Reason: {result.brief_explanation}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing doc {doc.metadata['source']}: {e}\")\n",
    "    \n",
    "    print(f\"--- [Distiller] Distilled context down to {len(relevant_docs)} documents. ---\")\n",
    "    return {\"distilled_docs\": relevant_docs}\n",
    "\n",
    "# Node 3: Generation\n",
    "def generation_node(state: RAGGraphState):\n",
    "    print(\"--- [Generator] Synthesizing final answer from distilled context... ---\")\n",
    "    context = format_docs(state['distilled_docs'])\n",
    "    answer = (generator_prompt | llm | StrOutputParser()).invoke({\"context\": context, \"question\": state['question']})\n",
    "    return {\"final_answer\": answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184dbc67",
   "metadata": {},
   "source": [
    "#### 3.2.2: Assembling the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73eed8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced RAG graph compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(RAGGraphState)\n",
    "workflow.add_node(\"retrieve\", retrieval_node)\n",
    "workflow.add_node(\"distill\", distill_context_node)\n",
    "workflow.add_node(\"generate\", generation_node)\n",
    "\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"distill\")\n",
    "workflow.add_edge(\"distill\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "advanced_rag_app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d8d77a",
   "metadata": {},
   "source": [
    "## Part 4: Head-to-Head Comparison\n",
    "\n",
    "Let's ask a specific question. The vector search will likely pull in documents about both the QLeap-V4 and the older QLeap-V3 due to semantic similarity. This is a classic scenario where context distillation is needed to prevent the generator from getting confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d00760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is the recommended power supply for the QLeap-V4 processor?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef63300",
   "metadata": {},
   "source": [
    "### 4.1: Running the Simple RAG System (Large Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059fdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================\n",
      "                SIMPLE RAG SYSTEM (LARGE CONTEXT)\n",
      "=============================================================\n",
      "\n",
      "--- Retrieved 10 Documents ---\n",
      "Context Size: 284 tokens\n",
      "\n",
      "--- Generation ---\n",
      "Generation Time: 7.89 seconds\n",
      "Final Answer:\n",
      "Based on the context, a power supply unit of at least 1200W is recommended for the QLeap-V4 processor. The QLeap-V3 chip had a recommended power supply of 800W.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"                SIMPLE RAG SYSTEM (LARGE CONTEXT)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "raw_docs_simple = retriever.invoke(user_query)\n",
    "context_simple = format_docs(raw_docs_simple)\n",
    "context_tokens_simple = count_tokens(context_simple)\n",
    "\n",
    "print(f\"--- Retrieved {len(raw_docs_simple)} Documents ---\")\n",
    "print(f\"Context Size: {context_tokens_simple} tokens\\n\")\n",
    "\n",
    "print(\"--- Generation ---\")\n",
    "gen_start_time = time.time()\n",
    "simple_answer = simple_rag_chain.invoke(user_query)\n",
    "gen_time_simple = time.time() - gen_start_time\n",
    "print(f\"Generation Time: {gen_time_simple:.2f} seconds\")\n",
    "print(\"Final Answer:\")\n",
    "print(simple_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ba143",
   "metadata": {},
   "source": [
    "### 4.2: Running the Advanced RAG System (Distilled Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553817d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================\n",
      "             ADVANCED RAG SYSTEM (DISTILLED CONTEXT)\n",
      "=============================================================\n",
      "\n",
      "--- [Retriever] Retrieving initial set of 10 documents... ---\n",
      "--- [Distiller] Pre-processing 10 raw documents in parallel... ---\n",
      "  - Doc 'QL-V4-HardwareGuide' IS relevant. Reason: The document directly states the recommended power supply for the QLeap-V4 processor.\n",
      "  - Doc 'QL-V3-HardwareGuide' is NOT relevant. Reason: The document is about the QLeap-V3, not the QLeap-V4.\n",
      "  - Doc 'QL-V4-SpecSheet' is NOT relevant. Reason: This document describes the QLeap-V4's use case but does not mention the power supply.\n",
      "  - Doc 'QL-V4-Pricing' is NOT relevant. Reason: The document discusses the price, not the power supply requirements.\n",
      "  - Doc 'QL-V3-KnownIssues' is NOT relevant. Reason: This document is about the previous generation chip, the QLeap-V3.\n",
      "  - Doc 'QL-V4-Troubleshooting' is NOT relevant. Reason: The document mentions the thermal system but not the power supply unit.\n",
      "  - Doc 'Project-Titan-FAQ' is NOT relevant. Reason: The document is about a different project, not the QLeap-V4 hardware requirements.\n",
      "  - Doc 'QL-V4-Warranty' is NOT relevant. Reason: The document describes the warranty policy, not hardware specifications.\n",
      "  - Doc 'Aura-TechSpec' is NOT relevant. Reason: The document is about the Aura Smart Ring, not the QLeap-V4 processor.\n",
      "  - Doc 'QL-V4-Downloads' is NOT relevant. Reason: This document is about software drivers, not the power supply.\n",
      "--- [Distiller] Distilled context down to 1 documents. ---\n",
      "Context Size: 29 tokens\n",
      "\n",
      "--- [Generator] Synthesizing final answer from distilled context... ---\n",
      "Generation Time: 2.15 seconds\n",
      "Final Answer:\n",
      "Based on the provided context, a power supply unit of at least 1200W is recommended for the QLeap-V4 processor.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"             ADVANCED RAG SYSTEM (DISTILLED CONTEXT)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "inputs = {\"question\": user_query}\n",
    "advanced_result = None\n",
    "for output in advanced_rag_app.stream(inputs, stream_mode=\"values\"):\n",
    "    advanced_result = output\n",
    "\n",
    "distilled_docs = advanced_result['distilled_docs']\n",
    "context_advanced = format_docs(distilled_docs)\n",
    "context_tokens_advanced = count_tokens(context_advanced)\n",
    "\n",
    "print(f\"Context Size: {context_tokens_advanced} tokens\\n\")\n",
    "\n",
    "# Manually time the final generation step for comparison\n",
    "print(\"--- [Generator] Synthesizing final answer from distilled context... ---\")\n",
    "gen_start_time = time.time()\n",
    "advanced_answer = (generator_prompt | llm | StrOutputParser()).invoke({\"context\": context_advanced, \"question\": user_query})\n",
    "gen_time_advanced = time.time() - gen_start_time\n",
    "print(f\"Generation Time: {gen_time_advanced:.2f} seconds\")\n",
    "print(\"Final Answer:\")\n",
    "print(advanced_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
