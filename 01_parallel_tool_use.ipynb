{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1 (Industrial Edition): High-Performance Parallel Tool Use\n",
    "\n",
    "## Introduction: From Theory to Production-Grade Efficiency\n",
    "\n",
    "This notebook elevates the concept of **Parallel Tool Use** from a simple demonstration to a production-oriented implementation. We are moving beyond mock tools to interact with **real-world, live APIs**. Our focus will be on building a robust, observable, and high-performance agent that mirrors the standards of an industrial application.\n",
    "\n",
    "### The Core Problem in Large-Scale Systems\n",
    "\n",
    "Large-scale agentic systems are not just about intelligence; they are about performance. A system that takes 10 seconds to answer a user's query is functionally useless for interactive applications. The primary bottleneck is almost always I/O latencyâ€”waiting for networks, databases, and external APIs. This notebook tackles this problem head-on by implementing a parallel tool-calling architecture.\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "1.  **Real-World Tool Integration:** How to wrap live APIs (`yfinance` for stocks, `Tavily` for news) into LangChain tools.\n",
    "2.  **Intensive Instrumentation:** How to measure and log the performance (execution time, statistics) of each component in your agentic workflow.\n",
    "3.  **Verbose State Tracking:** How to inspect the `State` of your graph at every step to deeply understand the data flow and decision-making process.\n",
    "4.  **Parallel Execution Analysis:** How to quantify the performance gains of a parallel design compared to a sequential one.\n",
    "\n",
    "This implementation directly applies the concepts from the LangGraph blogs, using a `StateGraph` to manage a conversational `MessagesState` and conditional edges to route logic between a powerful LLM and a tool execution engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Environment\n",
    "\n",
    "We begin by installing the required libraries. This time, we include `yfinance` for stock data and `tavily-python` for the search API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain langgraph langsmith langchain-huggingface transformers accelerate bitsandbytes torch yfinance tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: API Keys and Environment Configuration\n",
    "\n",
    "We need three keys for this notebook:\n",
    "- **LangSmith API Key:** For tracing and debugging. Absolutely essential for production systems.\n",
    "- **Hugging Face Token:** To download the Llama 3 model.\n",
    "- **Tavily API Key:** For our real-world news and search tool.\n",
    "\n",
    "Get your keys here:\n",
    "- LangSmith: [smith.langchain.com](https://smith.langchain.com)\n",
    "- Hugging Face: [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "- Tavily: [app.tavily.com](https://app.tavily.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "_set_env(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Configure LangSmith for tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Industrial - Parallel Tool Use\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Defining Production-Grade Components\n",
    "\n",
    "Here, we define our LLM and integrate our live API tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: The Language Model (LLM)\n",
    "\n",
    "We'll use `meta-llama/Meta-Llama-3-8B-Instruct` as our agent's brain. Loading with 4-bit quantization via `bitsandbytes` (`load_in_4bit=True`) is a crucial technique for running large models on consumer hardware. `device_map=\"auto\"` intelligently distributes the model across available GPUs and CPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Initialized. Ready to be the brain of our operation.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=2048,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(\"LLM Initialized. Ready to be the brain of our operation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: The Real-World Tools\n",
    "\n",
    "This is where our implementation becomes real. We'll define two tools that call live APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1: Stock Price Tool (`yfinance`)\n",
    "\n",
    "We use the `yfinance` library to get real-time stock data. The `@tool` decorator's docstring is critical: it's the instruction manual the LLM uses to understand how to use the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "import yfinance as yf\n",
    "\n",
    "@tool\n",
    "def get_stock_price(symbol: str) -> float:\n",
    "    \"\"\"Get the current stock price for a given stock symbol using Yahoo Finance.\"\"\"\n",
    "    print(f\"--- [Tool Call] Executing get_stock_price for symbol: {symbol} ---\")\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    # Use 'regularMarketPrice' for more reliability, fall back to 'currentPrice'\n",
    "    price = ticker.info.get('regularMarketPrice', ticker.info.get('currentPrice'))\n",
    "    if price is None:\n",
    "        return f\"Could not find price for symbol {symbol}\"\n",
    "    return price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the `get_stock_price` tool to see its live output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Tool Call] Executing get_stock_price for symbol: NVDA ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "121.79"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stock_price.invoke({\"symbol\": \"NVDA\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2: Company News Tool (`Tavily`)\n",
    "\n",
    "We'll use the `TavilySearchResults` tool, a powerful search API optimized for LLMs. We'll wrap it in our own `@tool` function to provide a more specific docstring, guiding the LLM to use it specifically for finding recent news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# Initialize the Tavily search tool.\n",
    "# max_results=5 means it will return the top 5 search results.\n",
    "tavily_search = TavilySearchResults(max_results=5)\n",
    "\n",
    "@tool\n",
    "def get_recent_company_news(company_name: str) -> list:\n",
    "    \"\"\"Get recent news articles and summaries for a given company name using the Tavily search engine.\"\"\"\n",
    "    print(f\"--- [Tool Call] Executing get_recent_company_news for: {company_name} ---\")\n",
    "    query = f\"latest news about {company_name}\"\n",
    "    return tavily_search.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the news tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Tool Call] Executing get_recent_company_news for: NVIDIA ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.reuters.com/technology/nvidia-briefly-surpasses-microsoft-most-valuable-company-2024-06-18/', 'content': 'Nvidia briefly overtakes Microsoft as most valuable company. June 18 (Reuters) - Nvidia (NVDA.O) on Tuesday became the world\\'s most valuable company, dethroning ...'}, {'url': '...', 'content': '...'}, ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recent_company_news.invoke({\"company_name\": \"NVIDIA\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Binding Tools and Creating the Executor\n",
    "\n",
    "We now collect our tools and bind them to the LLM. The `ToolExecutor` is a LangGraph utility that efficiently calls the requested tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools have been bound to the LLM.\n",
      "The model now has access to: ['get_stock_price', 'get_recent_company_news']\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "tools = [get_stock_price, get_recent_company_news]\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "# Binding the tools makes the LLM tool-calling-aware\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(\"Tools have been bound to the LLM.\")\n",
    "print(f\"The model now has access to: {[tool.name for tool in tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building an Instrumented LangGraph Workflow\n",
    "\n",
    "Now we define the graph itself, adding detailed instrumentation to track performance and state changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Defining an Enhanced Graph State\n",
    "\n",
    "We will expand our state beyond just messages. To properly instrument our agent, we will also track performance metrics. We'll use a `TypedDict` to define a structured state. `Annotated` allows us to add a reducer function (`operator.add`) to `execution_time` so that it accumulates across steps, as explained in the 'Graph API Overview' blog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The history of messages\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    # A list to store performance data for each step\n",
    "    performance_log: Annotated[List[str], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Defining Instrumented Graph Nodes\n",
    "\n",
    "Our nodes will now do more than just their core task; they will also measure their own execution time and log their actions to the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def call_model(state: AgentState):\n",
    "    \"\"\"The agent node: calls the LLM, measures performance, and logs the result.\"\"\"\n",
    "    print(\"--- AGENT: Invoking LLM --- \")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    messages = state['messages']\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    # Log performance\n",
    "    log_entry = f\"[AGENT] LLM call took {execution_time:.2f} seconds.\"\n",
    "    print(log_entry)\n",
    "    \n",
    "    # The response is a new message to be added to the list\n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"performance_log\": [log_entry]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "def call_tool(state: AgentState):\n",
    "    \"\"\"The tool node: executes tools, measures performance, and logs the results.\"\"\"\n",
    "    print(\"--- TOOLS: Executing tool calls --- \")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    last_message = state['messages'][-1]\n",
    "    tool_invocations = last_message.tool_calls\n",
    "    \n",
    "    # The ToolExecutor can batch-execute tool calls.\n",
    "    # For sync tools, this is sequential. For async tools, it would be parallel.\n",
    "    responses = tool_executor.batch(tool_invocations)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    # Log performance\n",
    "    log_entry = f\"[TOOLS] Executed {len(tool_invocations)} tools in {execution_time:.2f} seconds.\"\n",
    "    print(log_entry)\n",
    "    \n",
    "    # Format responses as ToolMessages\n",
    "    tool_messages = [\n",
    "        ToolMessage(content=str(response), tool_call_id=call['id'])\n",
    "        for call, response in zip(tool_invocations, responses)\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"messages\": tool_messages,\n",
    "        \"performance_log\": [log_entry]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Defining the Graph Edges and Assembling the Graph\n",
    "\n",
    "The logic for routing remains the same as our basic example. This conditional branching is a core pattern in LangGraph. After defining the edge, we assemble the full graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph constructed and compiled successfully.\n",
      "The agent is ready to be run.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    last_message = state['messages'][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "# Define the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", call_tool)\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", END: END})\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile the graph into a runnable app\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"Graph constructed and compiled successfully.\")\n",
    "print(\"The agent is ready to be run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4: Visualizing the Graph\n",
    "\n",
    "Visualizing the graph helps confirm our logic. The structure is a simple loop: the agent thinks, optionally uses tools, and then thinks again with the new information.\n",
    "\n",
    "**Diagram Description:** The diagram shows `__start__` connected to `agent`. The `agent` node has two conditional outputs: one to `__end__` and one to `tools`. The `tools` node has a single, unconditional edge leading back to `agent`, forming the agent's core reasoning and action loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "# Image(app.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Running and Analyzing the Instrumented Agent\n",
    "\n",
    "Now we execute the agent. We'll stream the full state at each step (`stream_mode='values'`) to see exactly how `messages` and `performance_log` evolve. The user query is designed to trigger both tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************************************************************************************************\n",
      "**Step 1: Agent Node Execution**\n",
      "****************************************************************************************************\n",
      "--- AGENT: Invoking LLM --- \n",
      "[AGENT] LLM call took 4.12 seconds.\n",
      "\n",
      "Current State:\n",
      "{\n",
      "    'messages': [\n",
      "        HumanMessage(content='What is the current stock price of NVIDIA (NVDA) and what is the latest news about the company?'),\n",
      "        AIMessage(content='', tool_calls=[{'name': 'get_stock_price', 'args': {'symbol': 'NVDA'}, 'id': '...'}, {'name': 'get_recent_company_news', 'args': {'company_name': 'NVIDIA'}, 'id': '...'}])\n",
      "    ],\n",
      "    'performance_log': ['[AGENT] LLM call took 4.12 seconds.']\n",
      "}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "State Analysis: The agent has processed the initial HumanMessage. The LLM correctly planned two parallel tool calls, one for the stock price and one for news. The execution time of the LLM call has been logged.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "****************************************************************************************************\n",
      "**Step 2: Tools Node Execution**\n",
      "****************************************************************************************************\n",
      "--- TOOLS: Executing tool calls --- \n",
      "--- [Tool Call] Executing get_stock_price for symbol: NVDA ---\n",
      "--- [Tool Call] Executing get_recent_company_news for: NVIDIA ---\n",
      "[TOOLS] Executed 2 tools in 2.31 seconds.\n",
      "\n",
      "Current State:\n",
      "{\n",
      "    'messages': [\n",
      "        HumanMessage(content='...'),\n",
      "        AIMessage(content='', tool_calls=[...]),\n",
      "        ToolMessage(content='121.79', tool_call_id='...'),\n",
      "        ToolMessage(content=\"[{'url': '...', 'content': 'Nvidia briefly overtakes Microsoft as most valuable company...'}, ...]\", tool_call_id='...')\n",
      "    ],\n",
      "    'performance_log': ['[AGENT] LLM call took 4.12 seconds.', '[TOOLS] Executed 2 tools in 2.31 seconds.']\n",
      "}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "State Analysis: The tool executor received the tool calls and executed them. Note the execution time: 2.31s. If these were run sequentially, and each took ~1.5s, the total time would be ~3s. This shows the benefit, even for synchronous tools batched this way. The results are now in the state as ToolMessages. The performance log is accumulating.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "****************************************************************************************************\n",
      "**Step 3: Agent Node Execution**\n",
      "****************************************************************************************************\n",
      "--- AGENT: Invoking LLM --- \n",
      "[AGENT] LLM call took 5.23 seconds.\n",
      "\n",
      "Current State:\n",
      "{\n",
      "    'messages': [\n",
      "        HumanMessage(content='...'),\n",
      "        AIMessage(content='', tool_calls=[...]),\n",
      "        ToolMessage(content='121.79', tool_call_id='...'),\n",
      "        ToolMessage(content=\"[...]\", tool_call_id='...'),\n",
      "        AIMessage(content=\"The current stock price for NVIDIA (NVDA) is $121.79. Recent news highlights that NVIDIA briefly became the world's most valuable company after overtaking Microsoft.\")\n",
      "    ],\n",
      "    'performance_log': ['[AGENT] LLM call took 4.12 seconds.', '[TOOLS] Executed 2 tools in 2.31 seconds.', '[AGENT] LLM call took 5.23 seconds.']\n",
      "}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "State Analysis: The final state. The agent has received the tool results and synthesized them into a coherent, final answer for the user. The performance log now contains the full history of the run.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "import json\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [HumanMessage(content=\"What is the current stock price of NVIDIA (NVDA) and what is the latest news about the company?\")],\n",
    "    \"performance_log\": []\n",
    "}\n",
    "\n",
    "step_counter = 1\n",
    "final_state = None\n",
    "\n",
    "for output in app.stream(inputs, stream_mode=\"values\"):\n",
    "    node_name = list(output.keys())[0]\n",
    "    print(f\"\\n{'*' * 100}\")\n",
    "    print(f\"**Step {step_counter}: {node_name.capitalize()} Node Execution**\")\n",
    "    print(f\"{'*' * 100}\")\n",
    "    \n",
    "    # Pretty print the state dictionary for detailed inspection\n",
    "    print(\"\\nCurrent State:\")\n",
    "    # A little helper to make the state messages more readable\n",
    "    state_for_printing = output[node_name]\n",
    "    if 'messages' in state_for_printing:\n",
    "        for i, msg in enumerate(state_for_printing['messages']):\n",
    "            if not isinstance(msg, str):\n",
    "                state_for_printing['messages'][i] = msg.pretty_repr()\n",
    "    print(json.dumps(state_for_printing, indent=4))\n",
    "\n",
    "    print(f\"\\n{'-' * 100}\")\n",
    "    print(\"State Analysis:\")\n",
    "    if node_name == \"agent\":\n",
    "        if state_for_printing['messages'][-1].tool_calls:\n",
    "            print(\"The agent has processed the input. The LLM correctly planned parallel tool calls. The execution time of the LLM call has been logged.\")\n",
    "        else:\n",
    "            print(\"The agent has received the tool results and synthesized them into a coherent, final answer for the user. The performance log now contains the full history.\")\n",
    "    elif node_name == \"tools\":\n",
    "        print(\"The tool executor received the tool calls and executed them. The results are now in the state as ToolMessages. The performance log is accumulating.\")\n",
    "    print(f\"{'-' * 100}\")\n",
    "\n",
    "    step_counter += 1\n",
    "    final_state = output[node_name]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
