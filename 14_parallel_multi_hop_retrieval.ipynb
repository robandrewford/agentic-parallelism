{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 14 (Industrial Edition): Parallel Multi-Hop Retrieval for Complex Questions\n",
    "\n",
    "## Introduction: From Question Answering to AI Research Assistant\n",
    "\n",
    "This notebook explores our final and most sophisticated RAG pattern: **Parallel Multi-Hop Retrieval**. This architecture elevates a RAG system from a simple fact-lookup tool into a genuine research agent capable of answering complex, comparative, and multi-step questions. It tackles queries that cannot be answered by any single document but require synthesizing information across multiple sources.\n",
    "\n",
    "### The Core Concept: Decompose, Retrieve in Parallel, and Synthesize\n",
    "\n",
    "The workflow mirrors how a human researcher would tackle a complex question:\n",
    "1.  **Decompose:** A high-level \"Meta-Agent\" analyzes the complex user query and breaks it down into several simpler, independent sub-questions.\n",
    "2.  **Scatter (Parallel Retrieval):** Each sub-question is dispatched to its own dedicated \"Retrieval Agent\". These agents run in parallel, each performing a standard RAG process to find the answer to its specific sub-question.\n",
    "3.  **Gather & Synthesize:** The Meta-Agent collects the answers to all the sub-questions and then performs a final reasoning step to synthesize them into a single, comprehensive answer to the original complex query.\n",
    "\n",
    "### Role in a Large-Scale System: Evolving RAG from Simple Q&A to Complex Research & Synthesis\n",
    "\n",
    "This pattern is the key to unlocking deep reasoning capabilities in any knowledge-intensive application:\n",
    "- **Financial Analysis:** Answering \"Compare the Q1 revenue growth of our top three competitors.\"\n",
    "- **Scientific Research:** Summarizing \"What is the relationship between protein A and disease B, and what are the known therapeutic interventions?\"\n",
    "- **Legal Strategy:** Analyzing \"What precedents exist for patent infringement cases involving software, and how do they differ from hardware cases?\"\n",
    "\n",
    "We will build and compare a Simple RAG system with a Multi-Hop RAG system. We will demonstrate that only the Multi-Hop system can successfully gather the necessary evidence to provide an **accurate and insightful** answer to a complex comparative question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain langgraph langsmith langchain-huggingface transformers accelerate bitsandbytes torch langchain-community sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: API Keys and Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "_set_env(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Industrial - RAG Multi-Hop Retrieval\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Components for the Multi-Hop System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: The Language Model (LLM)\n",
    "\n",
    "We will use `meta-llama/Meta-Llama-3-8B-Instruct` for all our agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Initialized. Ready to power our research agent.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", load_in_4bit=True)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=2048, do_sample=False)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(\"LLM Initialized. Ready to power our research agent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Creating the Knowledge Base\n",
    "\n",
    "We'll create a knowledge base with distinct, non-overlapping information about two different products. This will make it impossible for a single retrieval step to answer a comparative question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Base created with 4 documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "kb_docs = [\n",
    "    Document(page_content=\"The QLeap-V4 processor is designed for maximum performance in data centers. It consumes 1200W of power under full load and uses a specialized liquid cooling system to manage heat.\", metadata={\"product\": \"QLeap-V4\"}),\n",
    "    Document(page_content=\"Key features of the QLeap-V4 include 128 tensor cores and a 3nm process node, making it ideal for large-scale AI model training.\", metadata={\"product\": \"QLeap-V4\"}),\n",
    "    Document(page_content=\"The Eco-AI-M2 chip is designed for edge computing and mobile devices. Its primary feature is low power consumption, drawing only 15W under full load.\", metadata={\"product\": \"Eco-AI-M2\"}),\n",
    "    Document(page_content=\"Built on a 7nm process node, the Eco-AI-M2 has 8 specialized neural cores, making it perfect for real-time inference on devices like drones and smart cameras.\", metadata={\"product\": \"Eco-AI-M2\"})\n",
    "]\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(kb_docs, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "print(f\"Knowledge Base created with {len(kb_docs)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Structured Data Models\n",
    "\n",
    "We need Pydantic models to structure the decomposition step and to manage the flow of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class SubQuestions(BaseModel):\n",
    "    \"\"\"A list of independent sub-questions to be answered in parallel.\"\"\"\n",
    "    questions: List[str] = Field(description=\"A list of 2-3 simple, self-contained questions that, when answered together, will fully address the original complex query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Baseline - A Simple RAG System\n",
    "\n",
    "Let's first see how a standard RAG agent fails when faced with a complex, comparative question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "generator_prompt_template = (\n",
    "    \"You are an expert AI hardware analyst. Answer the user's question with high accuracy, based *only* on the following context. \"\n",
    "    \"Synthesize the information into a clear, comparative answer.\\n\\n\"\n",
    "    \"Context:\\n{context}\\n\\nQuestion: {question}\"\n",
    ")\n",
    "generator_prompt = ChatPromptTemplate.from_template(generator_prompt_template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "simple_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | generator_prompt\n",
    "    | llm\n",
    "| StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Building the Multi-Hop RAG Graph\n",
    "\n",
    "Now we build the advanced system. It will have nodes for decomposition, parallel retrieval, and final synthesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Graph State and Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Dict, Annotated\n",
    "from langchain_core.documents import Document\n",
    "import operator\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class MultiHopRAGState(TypedDict):\n",
    "    original_question: str\n",
    "    sub_questions: List[str]\n",
    "    # The dict will store the answer to each sub-question\n",
    "    sub_question_answers: Annotated[Dict[str, str], operator.update]\n",
    "    final_answer: str\n",
    "\n",
    "# Node 1: Decomposer (The Meta-Agent)\n",
    "decomposer_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a query decomposition expert. Your job is to break down a complex question into simple, independent sub-questions that can be answered by a retrieval system. \"\n",
    "    \"Do not try to answer the questions yourself.\\n\\n\"\n",
    "    \"Question: {question}\"\n",
    ")\n",
    "decomposer_chain = decomposer_prompt | llm.with_structured_output(SubQuestions)\n",
    "\n",
    "def decomposer_node(state: MultiHopRAGState):\n",
    "    print(\"--- [Meta-Agent] Decomposing complex question... ---\")\n",
    "    result = decomposer_chain.invoke({\"question\": state['original_question']})\n",
    "    print(f\"--- [Meta-Agent] Generated {len(result.questions)} sub-questions. ---\")\n",
    "    return {\"sub_questions\": result.questions}\n",
    "\n",
    "# Node 2: Parallel Retrieval Agents\n",
    "# This is a self-contained RAG chain that answers a single, simple question.\n",
    "sub_question_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | generator_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def retrieval_agent_node(state: MultiHopRAGState):\n",
    "    \"\"\"Runs a RAG process for each sub-question in parallel.\"\"\"\n",
    "    print(f\"--- [Retrieval Agents] Answering {len(state['sub_questions'])} sub-questions in parallel... ---\")\n",
    "    \n",
    "    answers = {}\n",
    "    with ThreadPoolExecutor(max_workers=len(state['sub_questions'])) as executor:\n",
    "        # Map each sub-question to the RAG chain\n",
    "        future_to_question = {executor.submit(sub_question_rag_chain.invoke, q): q for q in state['sub_questions']}\n",
    "        for future in as_completed(future_to_question):\n",
    "            question = future_to_question[future]\n",
    "            try:\n",
    "                answer = future.result()\n",
    "                answers[question] = answer\n",
    "                print(f\"  - Answer found for sub-question: '{question}'\")\n",
    "            except Exception as e:\n",
    "                answers[question] = f\"Error answering question: {e}\"\n",
    "\n",
    "    return {\"sub_question_answers\": answers}\n",
    "\n",
    "# Node 3: Synthesizer (The Meta-Agent's final step)\n",
    "synthesizer_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a synthesis expert. Your job is to combine the answers to several sub-questions into a single, cohesive, and comprehensive answer to the user's original complex question.\\n\\n\"\n",
    "    \"Original Question: {original_question}\\n\\n\"\n",
    "    \"Sub-Question Answers:\\n{sub_question_answers}\"\n",
    ")\n",
    "synthesizer_chain = synthesizer_prompt | llm | StrOutputParser()\n",
    "\n",
    "def synthesizer_node(state: MultiHopRAGState):\n",
    "    print(\"--- [Meta-Agent] Synthesizing final answer... ---\")\n",
    "    \n",
    "    sub_answers_str = \"\\n\".join([f\"- Q: {q}\\n- A: {a}\" for q, a in state['sub_question_answers'].items()])\n",
    "    \n",
    "    final_answer = synthesizer_chain.invoke({\n",
    "        \"original_question\": state['original_question'],\n",
    "        \"sub_question_answers\": sub_answers_str\n",
    "    })\n",
    "    return {\"final_answer\": final_answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Assembling the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Hop RAG graph compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(MultiHopRAGState)\n",
    "\n",
    "workflow.add_node(\"decompose\", decomposer_node)\n",
    "workflow.add_node(\"retrieve_in_parallel\", retrieval_agent_node)\n",
    "workflow.add_node(\"synthesize\", synthesizer_node)\n",
    "\n",
    "workflow.set_entry_point(\"decompose\")\n",
    "workflow.add_edge(\"decompose\", \"retrieve_in_parallel\")\n",
    "workflow.add_edge(\"retrieve_in_parallel\", \"synthesize\")\n",
    "workflow.add_edge(\"synthesize\", END)\n",
    "\n",
    "multi_hop_rag_app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Head-to-Head Comparison\n",
    "\n",
    "Let's ask our complex, comparative question. A single retrieval step will likely only find documents about one of the products, failing to gather the necessary context for a comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Compare the QLeap-V4 and the Eco-AI-M2, focusing on their target use case and power consumption.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1: Running the Simple RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================\n",
      "                  SIMPLE RAG SYSTEM OUTPUT\n",
      "=============================================================\n",
      "\n",
      "Retrieved Context:\n",
      "The Eco-AI-M2 chip is designed for edge computing and mobile devices. Its primary feature is low power consumption, drawing only 15W under full load.\n",
      "Built on a 7nm process node, the Eco-AI-M2 has 8 specialized neural cores, making it perfect for real-time inference on devices like drones and smart cameras.\n",
      "\n",
      "Final Answer:\n",
      "Based on the provided context, the Eco-AI-M2 chip is designed for edge computing and mobile devices, with a primary feature of low power consumption at only 15W under full load. The context does not contain information about the QLeap-V4, so I cannot provide a comparison.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"                  SIMPLE RAG SYSTEM OUTPUT\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# We intercept the retrieval step to inspect the documents\n",
    "simple_retrieved_docs = retriever.invoke(user_query)\n",
    "\n",
    "print(\"Retrieved Context:\")\n",
    "print(format_docs(simple_retrieved_docs) + \"\\n\")\n",
    "\n",
    "simple_answer = simple_rag_chain.invoke(user_query)\n",
    "print(\"Final Answer:\")\n",
    "print(simple_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2: Running the Multi-Hop RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Meta-Agent] Decomposing complex question... ---\n",
      "--- [Meta-Agent] Generated 2 sub-questions. ---\n",
      "--- [Retrieval Agents] Answering 2 sub-questions in parallel... ---\n",
      "  - Answer found for sub-question: 'What is the target use case and power consumption of the QLeap-V4?'\n",
      "  - Answer found for sub-question: 'What is the target use case and power consumption of the Eco-AI-M2?'\n",
      "--- [Meta-Agent] Synthesizing final answer... ---\n",
      "\n",
      "=============================================================\n",
      "                 MULTI-HOP RAG SYSTEM OUTPUT\n",
      "=============================================================\n",
      "\n",
      "--- Sub-Question Answers ---\n",
      "1. Q: What is the target use case and power consumption of the QLeap-V4?\n",
      "   A: The QLeap-V4 processor is designed for maximum performance in data centers, with a primary use case of large-scale AI model training. It consumes 1200W of power under full load.\n",
      "2. Q: What is the target use case and power consumption of the Eco-AI-M2?\n",
      "   A: The Eco-AI-M2 chip is designed for edge computing and mobile devices like drones and smart cameras. Its key feature is low power consumption, drawing only 15W under full load.\n",
      "\n",
      "--- Final Synthesized Answer ---\n",
      "The QLeap-V4 and the Eco-AI-M2 are designed for very different purposes, primarily distinguished by their target use case and power consumption.\n",
      "\n",
      "-   **QLeap-V4**: This is a high-performance processor intended for data centers. Its main use case is large-scale AI model training, and it has a high power consumption of 1200W.\n",
      "-   **Eco-AI-M2**: This is a low-power chip designed for edge computing and mobile devices. Its focus is on energy efficiency, consuming only 15W, making it suitable for applications like drones and smart cameras.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"original_question\": user_query}\n",
    "multi_hop_result = None\n",
    "for output in multi_hop_rag_app.stream(inputs, stream_mode=\"values\"):\n",
    "    multi_hop_result = output\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"                 MULTI-HOP RAG SYSTEM OUTPUT\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"--- Sub-Question Answers ---\")\n",
    "for i, (q, a) in enumerate(multi_hop_result['sub_question_answers'].items()):\n",
    "    print(f\"{i+1}. Q: {q}\")\n",
    "    print(f\"   A: {a}\")\n",
    "\n",
    "print(\"\\n--- Final Synthesized Answer ---\")\n",
    "print(multi_hop_result['final_answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
